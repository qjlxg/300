# stock_analysis_akshare_production.py - æœ€ç»ˆ AkShare ç¨³å®šç‰ˆ (è§£å†³ pytdx/baostock è¿æ¥åŠç™»å½•é—®é¢˜)

import pandas as pd
import pandas_ta as ta
from datetime import datetime, timedelta
import pytz
from concurrent.futures import ThreadPoolExecutor
import time 

# --- AkShare ä¾èµ– ---
import akshare as ak 

# --- é¡¶éƒ¨æ–°å¢å¯¼å…¥ ---
import logging
from pathlib import Path
from tqdm import tqdm
import warnings
warnings.filterwarnings("ignore")
warnings.simplefilter(action='ignore', category=FutureWarning)

# --- å¸¸é‡å’Œé…ç½® ---
shanghai_tz = pytz.timezone('Asia/Shanghai')
OUTPUT_DIR = "index_data" 
DEFAULT_START_DATE = '2000-01-01' 
INDICATOR_LOOKBACK_DAYS = 30 
LOCK_FILE = "stock_analysis.lock" 

MAX_WORKERS = 1 
MAX_RETRIES = 3 

# --- æŒ‡æ•°åˆ—è¡¨åŠä»£ç è½¬æ¢é€»è¾‘ (é’ˆå¯¹ AkShare) ---
# AkShare ç›´æ¥ä½¿ç”¨åŸå§‹ä»£ç æŸ¥è¯¢ï¼Œæ— éœ€å¸‚åœºå‰ç¼€ï¼Œä½†æˆ‘ä»¬ä¿ç•™åŸæœ‰ç»“æ„ã€‚

INDEX_LIST = {
    '000001': {'name': 'ä¸Šè¯æŒ‡æ•°', 'market': 1}, 
    '399001': {'name': 'æ·±è¯æˆæŒ‡', 'market': 0}, 
    '399006': {'name': 'åˆ›ä¸šæ¿æŒ‡', 'market': 0},
    '000016': {'name': 'ä¸Šè¯50', 'market': 1}, 
    '000300': {'name': 'æ²ªæ·±300', 'market': 1}, 
    '000905': {'name': 'ä¸­è¯500', 'market': 1},
    '000852': {'name': 'ä¸­è¯1000', 'market': 1}, 
    '000688': {'name': 'ç§‘åˆ›50', 'market': 1}, 
    '399300': {'name': 'æ²ªæ·±300(æ·±)', 'market': 0}, # æ³¨æ„ï¼šAkShareå¯èƒ½æ²¡æœ‰è¿™ä¸ªæ·±è¯çš„åˆ«åï¼Œä½†ä»£ç æ˜¯ä¸­è¯300
    '000991': {'name': 'ä¸­è¯å…¨æŒ‡', 'market': 1},
    '000906': {'name': 'ä¸­è¯800', 'market': 1}, 
    '399005': {'name': 'ä¸­å°æ¿æŒ‡', 'market': 0}, 
    '399330': {'name': 'æ·±è¯100', 'market': 0},
    '000010': {'name': 'ä¸Šè¯180', 'market': 1}, 
    '000015': {'name': 'çº¢åˆ©æŒ‡æ•°', 'market': 1},
    '000011': {'name': 'ä¸Šè¯åŸºé‡‘æŒ‡æ•°', 'market': 1}, 
    '399305': {'name': 'æ·±è¯åŸºé‡‘æŒ‡æ•°', 'market': 0}, 
    '399306': {'name': 'æ·±è¯ETFæŒ‡æ•°', 'market': 0},
}

# ç”³ä¸‡/ä¸­ä¿¡/ä¸‡å¾—è¡Œä¸šæŒ‡æ•° (AkShareæ”¯æŒç”³ä¸‡è¡Œä¸šæŒ‡æ•°ï¼Œå…¶ä½™éœ€è¦æ ¹æ®æ¥å£è°ƒæ•´ï¼Œæ­¤å¤„ä»…ä¿ç•™ç”³ä¸‡)
SW_INDUSTRY_DICT = {'801010':'å†œæ—ç‰§æ¸”','801020':'é‡‡æ˜','801030':'åŒ–å·¥','801040':'é’¢é“','801050':'æœ‰è‰²é‡‘å±','801080':'ç”µå­','801110':'å®¶ç”¨ç”µå™¨','801120':'é£Ÿå“é¥®æ–™','801130':'çººç»‡æœè£…','801140':'è½»å·¥åˆ¶é€ ','801150':'åŒ»è¯ç”Ÿç‰©','801160':'å…¬ç”¨äº‹ä¸š','801170':'äº¤é€šè¿è¾“','801180':'æˆ¿åœ°äº§','801200':'å•†ä¸šè´¸æ˜“','801210':'ä¼‘é—²æœåŠ¡','801230':'ç»¼åˆ','801710':'å»ºç­‘ææ–™','801720':'å»ºç­‘è£…é¥°','801730':'ç”µæ°”è®¾å¤‡','801740':'å›½é˜²å†›å·¥','801750':'è®¡ç®—æœº','801760':'ä¼ åª’','801770':'é€šä¿¡','801780':'é“¶è¡Œ','801790':'éé“¶é‡‘è','801880':'æ±½è½¦','801890':'æœºæ¢°è®¾å¤‡','801060':'å»ºç­‘å»ºæ','801070':'æœºæ¢°è®¾å¤‡','801090':'äº¤è¿è®¾å¤‡','801190':'é‡‘èæœåŠ¡','801100':'ä¿¡æ¯è®¾å¤‡','801220':'ä¿¡æ¯æœåŠ¡'}
CS_INDUSTRY_DICT = {} # æš‚æ—¶ç¦ç”¨ï¼Œè‹¥éœ€è¦ï¼Œéœ€æŸ¥AkShareæ¥å£
WIND_INDUSTRY_DICT = {} # æš‚æ—¶ç¦ç”¨

def get_pytdx_market(code): # ä¿æŒå‡½æ•°åï¼Œä½†ä»…ç”¨äºåˆ†ç±»
    code = str(code)
    if code.startswith('00') or code.startswith('88') or code.startswith('801') or code.startswith('CI005'):
        return 1  
    elif code.startswith('399'):
        return 0 
    return 1 

def merge_industry_indexes(index_list, industry_dict, prefix=""):
    for code, name in industry_dict.items():
        # AkShare ç”³ä¸‡æŒ‡æ•°ä½¿ç”¨ '801xxx.SI' æ ¼å¼ï¼Œä½†è¿™é‡Œä¼ å…¥çš„ code æ˜¯çº¯æ•°å­—
        pytdx_code = code.split('.')[0] 
        if pytdx_code not in index_list:
            index_list[pytdx_code] = {
                'name': f'{prefix}{name}',
                'market': get_pytdx_market(pytdx_code)
            }
    return index_list

INDEX_LIST = merge_industry_indexes(INDEX_LIST, SW_INDUSTRY_DICT, prefix="ç”³ä¸‡ä¸€çº§_")
INDEX_LIST = merge_industry_indexes(INDEX_LIST, CS_INDUSTRY_DICT, prefix="ä¸­ä¿¡ä¸€çº§_")
INDEX_LIST = merge_industry_indexes(INDEX_LIST, WIND_INDUSTRY_DICT, prefix="ä¸‡å¾—ä¸€çº§_")

# --- é…ç½®æ—¥å¿—ç³»ç»Ÿ (ä¿æŒä¸å˜) ---
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)s | %(message)s',
    handlers=[
        logging.FileHandler("stock_analysis.log", encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


# --- æŒ‡æ ‡è®¡ç®—å‡½æ•° (ä¿æŒä¸å˜) ---

def calculate_full_technical_indicators(df):
    """è®¡ç®—å®Œæ•´çš„æŠ€æœ¯æŒ‡æ ‡é›†ï¼šMA, RSI, KDJ, MACD, BBANDS, ATR, CCI, OBV"""
    if df.empty:
        return df
    
    # å¼ºåˆ¶å°† date åˆ—è®¾ç½®ä¸ºç´¢å¼•ï¼Œç¡®ä¿å®ƒæ˜¯ DatetimeIndex
    # AkShare è¿”å›çš„ date å·²ç»æ˜¯ str/datetimeï¼Œéœ€è¦è½¬æ¢ä¸º datetime å¯¹è±¡
    df['date'] = pd.to_datetime(df['date'])
    df = df.set_index('date')
    
    # ç¡®ä¿ä»·æ ¼å’Œæˆäº¤é‡åˆ—æ˜¯æ•°å­—ç±»å‹ (AkShareè¿”å›çš„å¯èƒ½æ˜¯objectï¼Œéœ€è¦è½¬æ¢)
    price_cols = ['open', 'close', 'high', 'low', 'volume']
    for col in price_cols:
        df[col] = pd.to_numeric(df[col], errors='coerce')
    
    # æŒ‡æ ‡è®¡ç®— (ä¿æŒä¸åŸè„šæœ¬ä¸€è‡´)
    df.ta.sma(length=5, append=True, col_names=('MA5',))
    df.ta.sma(length=20, append=True, col_names=('MA20',))
    df.ta.rsi(length=14, append=True, col_names=('RSI14',))
    df.ta.stoch(k=9, d=3, smooth_k=3, append=True) 
    df = df.rename(columns={'STOCHk_9_3_3': 'K', 'STOCHd_9_3_3': 'D', 'STOCHj_9_3_3': 'J'})
    df.ta.macd(append=True)
    df = df.rename(columns={'MACD_12_26_9': 'MACD', 'MACDh_12_26_9': 'MACDh', 'MACDs_12_26_9': 'MACDs'})
    df.ta.bbands(length=20, std=2, append=True)
    df = df.rename(columns={'BBL_20_2.0': 'BB_lower', 'BBM_20_2.0': 'BB_middle', 'BBU_20_2.0': 'BB_upper', 'BBB_20_2.0': 'BB_bandwidth', 'BBP_20_2.0': 'BB_percent'})
    df.ta.atr(length=14, append=True)
    df = df.rename(columns={'ATRr_14': 'ATR14'})
    df.ta.cci(length=20, append=True)
    df = df.rename(columns={'CCI_20_0.015': 'CCI20'})
    df.ta.obv(append=True)
    
    return df.reset_index()


def aggregate_and_analyze(df_raw_slice, freq, prefix):
    """æŒ‰é¢‘ç‡èšåˆæ•°æ®å¹¶è®¡ç®—æŒ‡æ ‡"""
    if df_raw_slice.empty:
        return pd.DataFrame()
        
    df_raw_slice['turnover_rate'] = float('nan') # AkShareæŒ‡æ•°æ•°æ®ä¸æä¾›æ¢æ‰‹ç‡
    
    # å°† index å¼ºåˆ¶è½¬ä¸º datetime å¯¹è±¡ indexï¼Œä»¥ä¾¿ resample
    # AkShare ç´¢å¼•å·²ç»æ˜¯ DatetimeIndexï¼Œä½†ä¿é™©èµ·è§å†æ¬¡è½¬æ¢
    df_raw_slice.index = pd.to_datetime(df_raw_slice.index)
    
    agg_df = df_raw_slice.resample(freq).agg({
        'open': 'first', 'high': 'max', 'low': 'min', 'close': 'last',
        'volume': 'sum', 'turnover_rate': 'mean'
    }).dropna(subset=['close'])
    
    if not agg_df.empty:
        agg_df = agg_df.reset_index().rename(columns={'index': 'date'})
        
        # å¼ºåˆ¶å°† datetime è½¬æ¢ä¸º date å¯¹è±¡ï¼Œä¿æŒæ ¼å¼ä¸€è‡´æ€§
        agg_df['date'] = agg_df['date'].dt.date 
        agg_df = calculate_full_technical_indicators(agg_df)
        
        cols_to_keep = agg_df.columns.drop(['date', 'open', 'close', 'high', 'low', 'volume', 'turnover_rate'])
        agg_df = agg_df[['date'] + cols_to_keep.tolist()]
        agg_df = agg_df.rename(columns={col: f'{col}_{prefix}' for col in cols_to_keep})
        agg_df.set_index('date', inplace=True)
        
    return agg_df

# --- å¢é‡æ•°æ®è·å–ä¸åˆ†ææ ¸å¿ƒå‡½æ•° (ä½¿ç”¨ AkShare) ---

def get_full_history_data(code, start_date_str):
    """
    ä½¿ç”¨ AkShare è·å–å®Œæ•´çš„å†å² K çº¿æ•°æ®ã€‚
    """
    logger.info(f"    - æ­£åœ¨é€šè¿‡ AkShare è·å– {code} (ä» {start_date_str} å¼€å§‹)...")

    # AkShareæŒ‡æ•°æ¥å£ï¼šindex_zh_a_hist 
    # period='daily', adjust='hfq' (åå¤æƒï¼Œå¯¹æŒ‡æ•°å½±å“ä¸å¤§ï¼Œä½†é€šå¸¸æ˜¯é»˜è®¤å‚æ•°)
    try:
        if code.startswith('801'):
            # AkShare ç”³ä¸‡ä¸€çº§è¡Œä¸šæŒ‡æ•°æ¥å£
            df = ak.index_industry_cons_sw(symbol=code, date="2024-07-31")
            # ç”³ä¸‡è¡Œä¸šæŒ‡æ•°æ•°æ®ç»“æ„å¤æ‚ï¼Œä¸” AkShare è·å–å…¶å†å²Kçº¿æ¥å£ä¸ç¨³å®šï¼Œå»ºè®®æš‚æ—¶ç¦ç”¨è¡Œä¸šæŒ‡æ•°
            logger.warning(f"    - è­¦å‘Šï¼šAkShare è¡Œä¸šæŒ‡æ•° {code} æ¥å£å¤æ‚æˆ–ä¸ç¨³å®šï¼Œè·³è¿‡ã€‚")
            return pd.DataFrame()
        else:
            # AkShare Aè‚¡æŒ‡æ•°æ¥å£
            df = ak.index_zh_a_hist(
                symbol=code, 
                period="daily", 
                start_date=start_date_str.replace('-', ''), # AkShareè¦æ±‚æ— è¿å­—ç¬¦çš„æ—¥æœŸ
                end_date=datetime.now().strftime('%Y%m%d')
            )

        if df.empty:
            logger.warning(f"    - AkShare æœªè¿”å› {code} æ•°æ®ã€‚")
            return pd.DataFrame()
            
        # å­—æ®µæ¸…æ´—ä¸é‡å‘½å
        df.rename(columns={
            'æ—¥æœŸ': 'date', 
            'å¼€ç›˜': 'open', 
            'æ”¶ç›˜': 'close', 
            'æœ€é«˜': 'high', 
            'æœ€ä½': 'low', 
            'æˆäº¤é‡': 'volume'
        }, inplace=True)
        
        # ä»…ä¿ç•™æ ¸å¿ƒåˆ—
        df = df[['date', 'open', 'close', 'high', 'low', 'volume']].copy()

        # ç¡®ä¿æ—¥æœŸæ˜¯ DatetimeIndex ä¾›åç»­å¤„ç†
        df['date'] = pd.to_datetime(df['date'])
        df.set_index('date', inplace=True)
        
        # ç¡®ä¿åˆ—æ˜¯ float ç±»å‹
        for col in ['open', 'close', 'high', 'low', 'volume']:
             df[col] = pd.to_numeric(df[col], errors='coerce')

        df.dropna(subset=['close'], inplace=True)
        
        # å°† DatetimeIndex è½¬æ¢ä¸º Date å¯¹è±¡ Index (ç”¨äºä¸æ—§æ•°æ®ç´¢å¼•ç±»å‹ç»Ÿä¸€)
        df.index = df.index.date
        df.sort_index(inplace=True)

        return df
    
    except Exception as e:
        logger.error(f"    - AkShare è·å– {code} å¤±è´¥ã€‚é”™è¯¯: {e}")
        return pd.DataFrame()


def get_and_analyze_data_slice(code, start_date):
    """è·å–æ•°æ®åˆ‡ç‰‡ï¼ŒåŒ…æ‹¬å…¨é‡è·å–ã€æœ¬åœ°ç­›é€‰å’ŒæŒ‡æ ‡è®¡ç®—ã€‚"""
    
    try:
        # 1. å…¨é‡è·å–æ•°æ® (AkShareæ¥å£é€šå¸¸æ˜¯å…¨é‡è·å–)
        # æ³¨æ„ï¼šè¿™é‡Œä¼ å…¥çš„ start_date æ˜¯ç”¨äº AkShare APIè¯·æ±‚çš„å‚æ•°
        df_full = get_full_history_data(code, start_date)

        if df_full.empty:
            logger.warning(f"    - {code} æœªè·å–åˆ°æœ‰æ•ˆæ•°æ®ã€‚")
            return None
            
        # 2. æœ¬åœ°ç­›é€‰ï¼ˆè·å–å¢é‡/é‡å åˆ‡ç‰‡ï¼‰
        # AkShare æ¥å£å·²é€šè¿‡ start_date ç­›é€‰ï¼Œè¿™é‡Œä¸éœ€è¦ä¸¥æ ¼ç­›é€‰ï¼Œä½†ä¿ç•™ä»£ç ç»“æ„
        df_raw = df_full.copy()
        
        # 3. å‡†å¤‡æŒ‡æ ‡è®¡ç®—
        df_raw_processed = df_raw.reset_index().rename(columns={'index': 'date'})

        # æ ¸å¿ƒä¿®æ­£ï¼šç¡®ä¿æ—¥æœŸæ˜¯ datetime ç±»å‹
        df_raw_processed['date'] = pd.to_datetime(df_raw_processed['date']) 
        
        # 4. æŒ‡æ ‡è®¡ç®—
        df_daily = calculate_full_technical_indicators(df_raw_processed.copy())
        
        # 5. å‘¨/æœˆ/å¹´æŒ‡æ ‡èšåˆè®¡ç®—
        # å‡†å¤‡ç”¨äº resample çš„ df_rawï¼Œéœ€è¦ DatetimeIndex
        df_raw.index = pd.to_datetime(df_raw.index)
        
        daily_cols = df_daily.columns.drop(['date', 'open', 'close', 'high', 'low', 'volume'])
        df_daily = df_daily.rename(columns={col: f'{col}_D' for col in daily_cols})
        df_daily.set_index('date', inplace=True)
        
        # èšåˆå’Œåˆå¹¶
        df_weekly = aggregate_and_analyze(df_raw, 'W', 'W')
        df_monthly = aggregate_and_analyze(df_raw, 'M', 'M')
        df_yearly = aggregate_and_analyze(df_raw, 'Y', 'Y')

        results = df_daily.copy()
        results = results.join(df_weekly, how='left').join(df_monthly, how='left').join(df_yearly, how='left')
        results.index.name = 'date'
        
        logger.info(f"    - {code} æˆåŠŸåˆ†æ {len(results)} è¡Œæ•°æ®åˆ‡ç‰‡ã€‚")
        return results.sort_index()

    except Exception as e:
        logger.error(f"    - é”™è¯¯ï¼šå¤„ç†æŒ‡æ•° {code} å¤±è´¥ã€‚æœ€ç»ˆé”™è¯¯: {e}")
        return None

# --- å•ä¸ªæŒ‡æ•°å¤„ç†å’Œä¿å­˜å‡½æ•° ---

def process_single_index(code_map):
    """å¤„ç†å•ä¸ªæŒ‡æ•°ï¼Œå®ç°å¢é‡ä¸‹è½½ã€è®¡ç®—å’Œè¦†ç›–ä¿å­˜"""
    code = code_map['code']
    name = code_map['name']
    
    logger.info(f"-> æ­£åœ¨å¤„ç†æŒ‡æ•°: {code} ({name})")
    
    file_name = f"{code.replace('.', '_')}.csv"
    output_path = Path(OUTPUT_DIR) / file_name
    
    start_date_to_request = DEFAULT_START_DATE
    df_old = pd.DataFrame()
    
    # 1. ç¡®å®šæœ¬æ¬¡ä¸‹è½½çš„èµ·å§‹æ—¥æœŸ 
    if output_path.exists():
        try:
            df_old = pd.read_csv(output_path, index_col='date', parse_dates=True)
            if not df_old.empty:
                latest_date_in_repo = df_old.index.max()
                
                # AkShare æ¥å£è¦æ±‚ä¼ å…¥çš„ start_date åŒ…å«æŒ‡æ ‡è®¡ç®—æ‰€éœ€çš„å†å²æ•°æ®
                start_date_for_calc = latest_date_in_repo - timedelta(days=INDICATOR_LOOKBACK_DAYS)
                start_date_to_request = start_date_for_calc.strftime('%Y-%m-%d')
                
                if start_date_for_calc.strftime('%Y-%m-%d') < DEFAULT_START_DATE:
                    start_date_to_request = DEFAULT_START_DATE
                
                logger.info(f"    - æ£€æµ‹åˆ°æ—§æ•°æ®ï¼Œæœ€æ–°æ—¥æœŸä¸º {latest_date_in_repo.strftime('%Y-%m-%d')}ã€‚API è¯·æ±‚ä» {start_date_to_request} å¼€å§‹çš„åˆ‡ç‰‡ï¼ˆå«é‡å ï¼‰ã€‚")
            else:
                logger.warning(f"    - æ—§æ–‡ä»¶ {output_path.name} ä¸ºç©ºï¼Œå°†å…¨é‡ä¸‹è½½ã€‚")
        except Exception as e:
            logger.error(f"    - è­¦å‘Šï¼šè¯»å–æ—§æ–‡ä»¶ {output_path.name} å¤±è´¥ ({e})ï¼Œå°†å…¨é‡ä¸‹è½½ã€‚")
            
    else:
        logger.info(f"    - æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°†å…¨é‡ä¸‹è½½ã€‚")


    # 2. è·å–æœ€æ–°æ•°æ®å’ŒæŒ‡æ ‡ (ä¸éœ€è¦ API å®¢æˆ·ç«¯ä¼ å…¥)
    df_new_analyzed = get_and_analyze_data_slice(code, start_date_to_request)
    
    if df_new_analyzed is None:
        is_today_updated = False
        if not df_old.empty and pd.api.types.is_datetime64_any_dtype(df_old.index):
             today = datetime.now(shanghai_tz).date()
             is_today_updated = df_old.index.max().date() == today
        
        if is_today_updated:
            logger.info(f"    - {code} æ•°æ®å·²æ˜¯ä»Šå¤©æœ€æ–°ï¼Œè·³è¿‡ä¿å­˜ã€‚")
        else:
            logger.warning(f"    - {code} æœªè·å–åˆ°æ–°æ•°æ®ï¼Œä¿æŒåŸæ–‡ä»¶ã€‚")
        return False

    # 3. æ•´åˆæ–°æ—§æ•°æ® 
    if not df_old.empty:
        # df_old çš„ç´¢å¼•æ˜¯ DatetimeIndexï¼Œè€Œ df_new_analyzed ç´¢å¼•æ˜¯ Date å¯¹è±¡ã€‚
        # è¿™é‡Œç»Ÿä¸€å°† df_old ç´¢å¼•è½¬æ¢ä¸º Date å¯¹è±¡è¿›è¡Œæ¯”è¾ƒã€‚
        df_old.index = df_old.index.date
        
        old_data_to_keep = df_old[df_old.index < df_new_analyzed.index.min()]
    else:
        old_data_to_keep = pd.DataFrame()
        
    # AkShare è¿”å›çš„æ•°æ®ç´¢å¼•æ˜¯ Date å¯¹è±¡ï¼Œéœ€è¦å°†å…¶è½¬æ¢å› DatetimeIndex æ‰èƒ½åˆå¹¶å’Œä¿å­˜
    df_new_analyzed.index = pd.to_datetime(df_new_analyzed.index)
    old_data_to_keep.index = pd.to_datetime(old_data_to_keep.index)


    df_combined = pd.concat([old_data_to_keep, df_new_analyzed])
    results_to_save = df_combined[~df_combined.index.duplicated(keep='last')]
    results_to_save = results_to_save.sort_index()

    logger.info(f"    - âœ… {code} æˆåŠŸæ›´æ–°ã€‚æ€»è¡Œæ•°: {len(results_to_save)}")
    
    # 4. ä¿å­˜åˆ° CSV 
    results_to_save.to_csv(output_path, encoding='utf-8')
    return True

# --- ä¸»æ‰§è¡Œé€»è¾‘ ---
def main():
    start_time = time.time()
    output_path = Path(OUTPUT_DIR)
    
    # 1. æ£€æŸ¥è¿è¡Œé”
    lock_file_path = Path(LOCK_FILE)
    if lock_file_path.exists():
        logger.warning("æ£€æµ‹åˆ°é”æ–‡ä»¶ï¼Œè„šæœ¬å¯èƒ½æ­£åœ¨è¿è¡Œæˆ–ä¸Šæ¬¡å¼‚å¸¸é€€å‡ºã€‚ç»ˆæ­¢æœ¬æ¬¡è¿è¡Œã€‚")
        return
    lock_file_path.touch() 
    
    # 2. AkShare æ— éœ€è¿æ¥/ç™»å½•ï¼Œç›´æ¥è¿›å…¥å¤„ç†
    logger.info("â€”" * 50)
    logger.info("ğŸš€ è„šæœ¬å¼€å§‹è¿è¡Œ (ä½¿ç”¨ AkShare)")
    
    try:
        # 3. åˆå§‹åŒ–ç›®å½•
        output_path.mkdir(exist_ok=True) 
        logger.info(f"ç»“æœå°†ä¿å­˜åˆ°ä¸“ç”¨ç›®å½•: {output_path.resolve()}")
        logger.info(f"å‡†å¤‡ä¸²è¡Œå¤„ç† {len(INDEX_LIST)} ä¸ªæŒ‡æ•°...")

        successful = 0
        failed = 0
        
        # 4. è½¬æ¢ INDEX_LIST æ ¼å¼ä»¥æ–¹ä¾¿å¤„ç†
        jobs = [{'code': code, **data} for code, data in INDEX_LIST.items()]
        
        # 5. ä½¿ç”¨ ThreadPoolExecutor è¿›è¡Œä¸²è¡Œå¤„ç† (MAX_WORKERS = 1)
        # å»ºè®®ä¸²è¡Œï¼Œé¿å…é¢‘ç¹è¯·æ±‚ AkShare å¯¼è‡´ IP è¢«å°ç¦
        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            
            futures = {
                executor.submit(process_single_index, job): job
                for job in jobs
            }
            
            for future in tqdm(futures, desc="å¤„ç†æŒ‡æ•°", unit="ä¸ª", ncols=100, leave=True):
                job = futures[future]
                try:
                    if future.result():
                        successful += 1
                    else:
                        failed += 1
                except Exception as e:
                    logger.error(f"å¤„ç† {job['code']} ({job['name']}) æ—¶å‘ç”Ÿæœªæ•è·å¼‚å¸¸: {e}")
                    failed += 1
        
        end_time = time.time()
        elapsed_time = end_time - start_time
        
        # 6. æœ€ç»ˆç»Ÿè®¡å’Œè¾“å‡º
        logger.info("â€”" * 50)
        logger.info(f"âœ… æ‰€æœ‰æŒ‡æ•°æ•°æ®å¤„ç†å®Œæˆã€‚æ€»è€—æ—¶: {elapsed_time:.2f} ç§’")
        logger.info(f"ç»Ÿè®¡ï¼šæˆåŠŸæ›´æ–° {successful} ä¸ªæ–‡ä»¶ï¼Œå¤±è´¥/è·³è¿‡ {failed} ä¸ªã€‚")

    finally:
        # 7. ç§»é™¤é”æ–‡ä»¶
        lock_file_path.unlink(missing_ok=True)
        logger.info("é”æ–‡ä»¶å·²æ¸…é™¤ã€‚")

if __name__ == "__main__":
    main()
