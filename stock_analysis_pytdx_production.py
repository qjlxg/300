# stock_analysis_akshare_production.py - V9 ç»ˆæç¨³å®šç‰ˆ (å¼ºåŒ–è¶…æ—¶ä¸å»¶è¿Ÿ)

import pandas as pd
import pandas_ta as ta
from datetime import datetime, timedelta
import pytz
from concurrent.futures import ThreadPoolExecutor
import time 

# --- AkShare ä¾èµ– ---
import akshare as ak 

# --- é¡¶éƒ¨æ–°å¢å¯¼å…¥ ---
import logging
from pathlib import Path
from tqdm import tqdm
import warnings
warnings.filterwarnings("ignore")
warnings.simplefilter(action='ignore', category=FutureWarning)


# --- AkShare å…¨å±€é…ç½® (V9 æ ¸å¿ƒæ”¹è¿› 1: è®¾ç½®å…¨å±€è¶…æ—¶) ---
# å¼ºåˆ¶è®¾ç½®å…¨å±€è¯·æ±‚è¶…æ—¶æ—¶é—´ä¸º 30 ç§’
try:
    ak.set_time_out(30)
except Exception as e:
    # å…¼å®¹æ€§å¤„ç†
    print(f"è­¦å‘Šï¼šè®¾ç½® AkShare å…¨å±€è¶…æ—¶å¤±è´¥ï¼š{e}")


# --- å¸¸é‡å’Œé…ç½® (V9 æ ¸å¿ƒæ”¹è¿› 2: æé«˜åŸºç¡€å»¶è¿Ÿ) ---
shanghai_tz = pytz.timezone('Asia/Shanghai')
OUTPUT_DIR = "index_data" 
DEFAULT_START_DATE = '2000-01-01' 
INDICATOR_LOOKBACK_DAYS = 30 
LOCK_FILE = "stock_analysis.lock" 

MAX_WORKERS = 1 # ä¿æŒä¸º1ï¼Œç¡®ä¿ä¸²è¡Œ
MAX_RETRIES = 5 # æœ€å¤§çš„é‡è¯•æ¬¡æ•°
BASE_DELAY = 10  # åŸºç¡€å»¶è¿Ÿæé«˜åˆ° 10 ç§’

# --- æŒ‡æ•°åˆ—è¡¨åŠä»£ç ç»“æ„ (ä¿æŒä¸å˜) ---

INDEX_LIST = {
    '000001': {'name': 'ä¸Šè¯æŒ‡æ•°', 'market': 1}, 
    '399001': {'name': 'æ·±è¯æˆæŒ‡', 'market': 0}, 
    '399006': {'name': 'åˆ›ä¸šæ¿æŒ‡', 'market': 0},
    '000016': {'name': 'ä¸Šè¯50', 'market': 1}, 
    '000300': {'name': 'æ²ªæ·±300', 'market': 1}, 
    '000905': {'name': 'ä¸­è¯500', 'market': 1},
    '000852': {'name': 'ä¸­è¯1000', 'market': 1}, 
    '000688': {'name': 'ç§‘åˆ›50', 'market': 1}, 
    '399300': {'name': 'æ²ªæ·±300(æ·±)', 'market': 0},
    '000991': {'name': 'ä¸­è¯å…¨æŒ‡', 'market': 1},
    '000906': {'name': 'ä¸­è¯800', 'market': 1}, 
    '399005': {'name': 'ä¸­å°æ¿æŒ‡', 'market': 0}, 
    '399330': {'name': 'æ·±è¯100', 'market': 0},
    '000010': {'name': 'ä¸Šè¯180', 'market': 1}, 
    '000015': {'name': 'çº¢åˆ©æŒ‡æ•°', 'market': 1},
    '000011': {'name': 'ä¸Šè¯åŸºé‡‘æŒ‡æ•°', 'market': 1}, 
    '399305': {'name': 'æ·±è¯åŸºé‡‘æŒ‡æ•°', 'market': 0}, 
    '399306': {'name': 'æ·±è¯ETFæŒ‡æ•°', 'market': 0},
}

# ç”³ä¸‡è¡Œä¸šæŒ‡æ•° (ä¸­ä¿¡/ä¸‡å¾—æš‚æ—¶ç¦ç”¨ï¼ŒAkShareç”³ä¸‡æ¥å£ç›¸å¯¹ç¨³å®šï¼Œä½†ä»éœ€è°¨æ…)
SW_INDUSTRY_DICT = {'801010':'å†œæ—ç‰§æ¸”','801020':'é‡‡æ˜','801030':'åŒ–å·¥','801040':'é’¢é“','801050':'æœ‰è‰²é‡‘å±','801080':'ç”µå­','801110':'å®¶ç”¨ç”µå™¨','801120':'é£Ÿå“é¥®æ–™','801130':'çººç»‡æœè£…','801140':'è½»å·¥åˆ¶é€ ','801150':'åŒ»è¯ç”Ÿç‰©','801160':'å…¬ç”¨äº‹ä¸š','801170':'äº¤é€šè¿è¾“','801180':'æˆ¿åœ°äº§','801200':'å•†ä¸šè´¸æ˜“','801210':'ä¼‘é—²æœåŠ¡','801230':'ç»¼åˆ','801710':'å»ºç­‘ææ–™','801720':'å»ºç­‘è£…é¥°','801730':'ç”µæ°”è®¾å¤‡','801740':'å›½é˜²å†›å·¥','801750':'è®¡ç®—æœº','801760':'ä¼ åª’','801770':'é€šä¿¡','801780':'é“¶è¡Œ','801790':'éé“¶é‡‘è','801880':'æ±½è½¦','801890':'æœºæ¢°è®¾å¤‡','801060':'å»ºç­‘å»ºæ','801070':'æœºæ¢°è®¾å¤‡','801090':'äº¤è¿è®¾å¤‡','801190':'é‡‘èæœåŠ¡','801100':'ä¿¡æ¯è®¾å¤‡','801220':'ä¿¡æ¯æœåŠ¡'}
CS_INDUSTRY_DICT = {} 
WIND_INDUSTRY_DICT = {} 

def get_pytdx_market(code): 
    code = str(code)
    if code.startswith('00') or code.startswith('88') or code.startswith('801') or code.startswith('CI005'):
        return 1  
    elif code.startswith('399'):
        return 0 
    return 1 

def merge_industry_indexes(index_list, industry_dict, prefix=""):
    for code, name in industry_dict.items():
        pytdx_code = code.split('.')[0] 
        if pytdx_code not in index_list:
            index_list[pytdx_code] = {
                'name': f'{prefix}{name}',
                'market': get_pytdx_market(pytdx_code)
            }
    return index_list

INDEX_LIST = merge_industry_indexes(INDEX_LIST, SW_INDUSTRY_DICT, prefix="ç”³ä¸‡ä¸€çº§_")
INDEX_LIST = merge_industry_indexes(INDEX_LIST, CS_INDUSTRY_DICT, prefix="ä¸­ä¿¡ä¸€çº§_")
INDEX_LIST = merge_industry_indexes(INDEX_LIST, WIND_INDUSTRY_DICT, prefix="ä¸‡å¾—ä¸€çº§_")

# --- é…ç½®æ—¥å¿—ç³»ç»Ÿ (ä¿æŒä¸å˜) ---
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)s | %(message)s',
    handlers=[
        logging.FileHandler("stock_analysis.log", encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


# --- æŒ‡æ ‡è®¡ç®—å‡½æ•° (ä¿æŒä¸å˜) ---

def calculate_full_technical_indicators(df):
    """è®¡ç®—å®Œæ•´çš„æŠ€æœ¯æŒ‡æ ‡é›†ï¼šMA, RSI, KDJ, MACD, BBANDS, ATR, CCI, OBV"""
    if df.empty:
        return df
    
    df['date'] = pd.to_datetime(df['date'])
    df = df.set_index('date')
    
    price_cols = ['open', 'close', 'high', 'low', 'volume']
    for col in price_cols:
        df[col] = pd.to_numeric(df[col], errors='coerce')
    
    df.ta.sma(length=5, append=True, col_names=('MA5',))
    df.ta.sma(length=20, append=True, col_names=('MA20',))
    df.ta.rsi(length=14, append=True, col_names=('RSI14',))
    df.ta.stoch(k=9, d=3, smooth_k=3, append=True) 
    df = df.rename(columns={'STOCHk_9_3_3': 'K', 'STOCHd_9_3_3': 'D', 'STOCHj_9_3_3': 'J'})
    df.ta.macd(append=True)
    df = df.rename(columns={'MACD_12_26_9': 'MACD', 'MACDh_12_26_9': 'MACDh', 'MACDs_12_26_9': 'MACDs'})
    df.ta.bbands(length=20, std=2, append=True)
    df = df.rename(columns={'BBL_20_2.0': 'BB_lower', 'BBM_20_2.0': 'BB_middle', 'BBU_20_2.0': 'BB_upper', 'BBB_20_2.0': 'BB_bandwidth', 'BBP_20_2.0': 'BB_percent'})
    df.ta.atr(length=14, append=True)
    df = df.rename(columns={'ATRr_14': 'ATR14'})
    df.ta.cci(length=20, append=True)
    df = df.rename(columns={'CCI_20_0.015': 'CCI20'})
    df.ta.obv(append=True)
    
    return df.reset_index()


def aggregate_and_analyze(df_raw_slice, freq, prefix):
    """æŒ‰é¢‘ç‡èšåˆæ•°æ®å¹¶è®¡ç®—æŒ‡æ ‡"""
    if df_raw_slice.empty:
        return pd.DataFrame()
        
    df_raw_slice['turnover_rate'] = float('nan') 
    
    df_raw_slice.index = pd.to_datetime(df_raw_slice.index)
    
    agg_df = df_raw_slice.resample(freq).agg({
        'open': 'first', 'high': 'max', 'low': 'min', 'close': 'last',
        'volume': 'sum', 'turnover_rate': 'mean'
    }).dropna(subset=['close'])
    
    if not agg_df.empty:
        agg_df = agg_df.reset_index().rename(columns={'index': 'date'})
        
        agg_df['date'] = agg_df['date'].dt.date 
        agg_df = calculate_full_technical_indicators(agg_df)
        
        cols_to_keep = agg_df.columns.drop(['date', 'open', 'close', 'high', 'low', 'volume', 'turnover_rate'])
        agg_df = agg_df.rename(columns={col: f'{col}_{prefix}' for col in cols_to_keep})
        agg_df.set_index('date', inplace=True)
        
    return agg_df

# --- å¢é‡æ•°æ®è·å–ä¸åˆ†ææ ¸å¿ƒå‡½æ•° (ä½¿ç”¨ AkShare V9 - å¼ºåŒ–é‡è¯•/å»¶è¿Ÿ) ---

def get_full_history_data(code, start_date_str):
    """
    ä½¿ç”¨ AkShare è·å–å®Œæ•´çš„å†å² K çº¿æ•°æ®ï¼Œå¹¶åŠ å…¥é‡è¯•å’Œå»¶è¿Ÿæœºåˆ¶ã€‚
    """
    logger.info(f"    - æ­£åœ¨é€šè¿‡ AkShare è·å– {code} (ä» {start_date_str} å¼€å§‹)...")
    
    for attempt in range(MAX_RETRIES):
        try:
            if code.startswith('801'):
                logger.warning(f"    - è­¦å‘Šï¼šAkShare è¡Œä¸šæŒ‡æ•° {code} æ¥å£å¤æ‚æˆ–ä¸ç¨³å®šï¼Œè·³è¿‡ã€‚")
                return pd.DataFrame()
            else:
                # AkShare Aè‚¡æŒ‡æ•°æ¥å£
                df = ak.index_zh_a_hist(
                    symbol=code, 
                    period="daily", 
                    start_date=start_date_str.replace('-', ''), # AkShareè¦æ±‚æ— è¿å­—ç¬¦çš„æ—¥æœŸ
                    end_date=datetime.now().strftime('%Y%m%d')
                )

            if df.empty:
                raise ValueError("AkShare returned an empty DataFrame.")
            
            # å­—æ®µæ¸…æ´—ä¸é‡å‘½å
            df.rename(columns={
                'æ—¥æœŸ': 'date', 
                'å¼€ç›˜': 'open', 
                'æ”¶ç›˜': 'close', 
                'æœ€é«˜': 'high', 
                'æœ€ä½': 'low', 
                'æˆäº¤é‡': 'volume'
            }, inplace=True)
            
            df = df[['date', 'open', 'close', 'high', 'low', 'volume']].copy()

            df['date'] = pd.to_datetime(df['date'])
            df.set_index('date', inplace=True)
            
            for col in ['open', 'close', 'high', 'low', 'volume']:
                 df[col] = pd.to_numeric(df[col], errors='coerce')

            df.dropna(subset=['close'], inplace=True)
            
            df.index = df.index.date
            df.sort_index(inplace=True)

            logger.info(f"    - âœ… {code} æ•°æ®è·å–æˆåŠŸ (å°è¯•æ¬¡æ•°: {attempt + 1})ã€‚")
            return df
        
        except Exception as e:
            # æ•è·è¿æ¥ä¸­æ–­ã€è¶…æ—¶ã€ç©ºæ•°æ®ç­‰æ‰€æœ‰é”™è¯¯
            logger.warning(f"    - AkShare è·å– {code} å¤±è´¥ (å°è¯• {attempt + 1}/{MAX_RETRIES})ã€‚é”™è¯¯: {e}")
            if attempt < MAX_RETRIES - 1:
                # å¢åŠ ç­‰å¾…æ—¶é—´ï¼Œå¹¶ä¹˜ä»¥å°è¯•æ¬¡æ•°ï¼Œå®ç°æŒ‡æ•°é€€é¿
                # V9 ç­‰å¾…æ—¶é—´ï¼š10s, 12s, 14s, 16s, 18s
                wait_time = BASE_DELAY + attempt * 2 
                logger.info(f"    - æ­£åœ¨ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                time.sleep(wait_time)
            else:
                logger.error(f"    - AkShare è·å– {code} æœ€ç»ˆå¤±è´¥ï¼Œæ”¾å¼ƒã€‚")
                return pd.DataFrame()


def get_and_analyze_data_slice(code, start_date):
    """è·å–æ•°æ®åˆ‡ç‰‡ï¼ŒåŒ…æ‹¬å…¨é‡è·å–ã€æœ¬åœ°ç­›é€‰å’ŒæŒ‡æ ‡è®¡ç®—ã€‚"""
    
    try:
        df_full = get_full_history_data(code, start_date)

        if df_full.empty:
            logger.warning(f"    - {code} æœªè·å–åˆ°æœ‰æ•ˆæ•°æ®ã€‚")
            return None
            
        df_raw = df_full.copy()
        
        df_raw_processed = df_raw.reset_index().rename(columns={'index': 'date'})
        df_raw_processed['date'] = pd.to_datetime(df_raw_processed['date']) 
        
        df_daily = calculate_full_technical_indicators(df_raw_processed.copy())
        
        df_raw.index = pd.to_datetime(df_raw.index)
        
        daily_cols = df_daily.columns.drop(['date', 'open', 'close', 'high', 'low', 'volume'])
        df_daily = df_daily.rename(columns={col: f'{col}_D' for col in daily_cols})
        df_daily.set_index('date', inplace=True)
        
        df_weekly = aggregate_and_analyze(df_raw, 'W', 'W')
        df_monthly = aggregate_and_analyze(df_raw, 'M', 'M')
        df_yearly = aggregate_and_analyze(df_raw, 'Y', 'Y')

        results = df_daily.copy()
        results = results.join(df_weekly, how='left').join(df_monthly, how='left').join(df_yearly, how='left')
        results.index.name = 'date'
        
        logger.info(f"    - {code} æˆåŠŸåˆ†æ {len(results)} è¡Œæ•°æ®åˆ‡ç‰‡ã€‚")
        return results.sort_index()

    except Exception as e:
        logger.error(f"    - é”™è¯¯ï¼šå¤„ç†æŒ‡æ•° {code} å¤±è´¥ã€‚æœ€ç»ˆé”™è¯¯: {e}")
        return None

# --- å•ä¸ªæŒ‡æ•°å¤„ç†å’Œä¿å­˜å‡½æ•° ---

def process_single_index(code_map):
    """å¤„ç†å•ä¸ªæŒ‡æ•°ï¼Œå®ç°å¢é‡ä¸‹è½½ã€è®¡ç®—å’Œè¦†ç›–ä¿å­˜"""
    code = code_map['code']
    name = code_map['name']
    
    logger.info(f"-> æ­£åœ¨å¤„ç†æŒ‡æ•°: {code} ({name})")
    
    file_name = f"{code.replace('.', '_')}.csv"
    output_path = Path(OUTPUT_DIR) / file_name
    
    start_date_to_request = DEFAULT_START_DATE
    df_old = pd.DataFrame()
    
    # 1. ç¡®å®šæœ¬æ¬¡ä¸‹è½½çš„èµ·å§‹æ—¥æœŸ 
    if output_path.exists():
        try:
            df_old = pd.read_csv(output_path, index_col='date', parse_dates=True)
            if not df_old.empty:
                latest_date_in_repo = df_old.index.max()
                
                start_date_for_calc = latest_date_in_repo - timedelta(days=INDICATOR_LOOKBACK_DAYS)
                start_date_to_request = start_date_for_calc.strftime('%Y-%m-%d')
                
                if start_date_for_calc.strftime('%Y-%m-%d') < DEFAULT_START_DATE:
                    start_date_to_request = DEFAULT_START_DATE
                
                logger.info(f"    - æ£€æµ‹åˆ°æ—§æ•°æ®ï¼Œæœ€æ–°æ—¥æœŸä¸º {latest_date_in_repo.strftime('%Y-%m-%d')}ã€‚API è¯·æ±‚ä» {start_date_to_request} å¼€å§‹çš„åˆ‡ç‰‡ï¼ˆå«é‡å ï¼‰ã€‚")
            else:
                logger.warning(f"    - æ—§æ–‡ä»¶ {output_path.name} ä¸ºç©ºï¼Œå°†å…¨é‡ä¸‹è½½ã€‚")
        except Exception as e:
            logger.error(f"    - è­¦å‘Šï¼šè¯»å–æ—§æ–‡ä»¶ {output_path.name} å¤±è´¥ ({e})ï¼Œå°†å…¨é‡ä¸‹è½½ã€‚")
            
    else:
        logger.info(f"    - æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°†å…¨é‡ä¸‹è½½ã€‚")


    # 2. è·å–æœ€æ–°æ•°æ®å’ŒæŒ‡æ ‡ 
    df_new_analyzed = get_and_analyze_data_slice(code, start_date_to_request)
    
    if df_new_analyzed is None:
        is_today_updated = False
        if not df_old.empty and pd.api.types.is_datetime64_any_dtype(df_old.index):
             today = datetime.now(shanghai_tz).date()
             is_today_updated = df_old.index.max().date() == today
        
        if is_today_updated:
            logger.info(f"    - {code} æ•°æ®å·²æ˜¯ä»Šå¤©æœ€æ–°ï¼Œè·³è¿‡ä¿å­˜ã€‚")
        else:
            logger.warning(f"    - {code} æœªè·å–åˆ°æ–°æ•°æ®ï¼Œä¿æŒåŸæ–‡ä»¶ã€‚")
        return False

    # 3. æ•´åˆæ–°æ—§æ•°æ® 
    if not df_old.empty:
        df_old.index = df_old.index.date
        old_data_to_keep = df_old[df_old.index < df_new_analyzed.index.min()]
    else:
        old_data_to_keep = pd.DataFrame()
        
    df_new_analyzed.index = pd.to_datetime(df_new_analyzed.index)
    old_data_to_keep.index = pd.to_datetime(old_data_to_keep.index)


    df_combined = pd.concat([old_data_to_keep, df_new_analyzed])
    results_to_save = df_combined[~df_combined.index.duplicated(keep='last')]
    results_to_save = results_to_save.sort_index()

    logger.info(f"    - âœ… {code} æˆåŠŸæ›´æ–°ã€‚æ€»è¡Œæ•°: {len(results_to_save)}")
    
    # 4. ä¿å­˜åˆ° CSV 
    results_to_save.to_csv(output_path, encoding='utf-8')
    return True

# --- ä¸»æ‰§è¡Œé€»è¾‘ ---
def main():
    start_time = time.time()
    output_path = Path(OUTPUT_DIR)
    
    lock_file_path = Path(LOCK_FILE)
    if lock_file_path.exists():
        logger.warning("æ£€æµ‹åˆ°é”æ–‡ä»¶ï¼Œè„šæœ¬å¯èƒ½æ­£åœ¨è¿è¡Œæˆ–ä¸Šæ¬¡å¼‚å¸¸é€€å‡ºã€‚ç»ˆæ­¢æœ¬æ¬¡è¿è¡Œã€‚")
        return
    lock_file_path.touch() 
    
    logger.info("â€”" * 50)
    logger.info("ğŸš€ è„šæœ¬å¼€å§‹è¿è¡Œ (ä½¿ç”¨ AkShare V9 - ç»ˆæç¨³å®šç‰ˆ)")
    
    try:
        output_path.mkdir(exist_ok=True) 
        logger.info(f"ç»“æœå°†ä¿å­˜åˆ°ä¸“ç”¨ç›®å½•: {output_path.resolve()}")
        logger.info(f"å‡†å¤‡ä¸²è¡Œå¤„ç† {len(INDEX_LIST)} ä¸ªæŒ‡æ•°...")

        successful = 0
        failed = 0
        
        jobs = [{'code': code, **data} for code, data in INDEX_LIST.items()]
        
        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            
            futures = {
                executor.submit(process_single_index, job): job
                for job in jobs
            }
            
            for future in tqdm(futures, desc="å¤„ç†æŒ‡æ•°", unit="ä¸ª", ncols=100, leave=True):
                job = futures[future]
                try:
                    if future.result():
                        successful += 1
                    else:
                        failed += 1
                except Exception as e:
                    logger.error(f"å¤„ç† {job['code']} ({job['name']}) æ—¶å‘ç”Ÿæœªæ•è·å¼‚å¸¸: {e}")
                    failed += 1
        
        end_time = time.time()
        elapsed_time = end_time - start_time
        
        logger.info("â€”" * 50)
        logger.info(f"âœ… æ‰€æœ‰æŒ‡æ•°æ•°æ®å¤„ç†å®Œæˆã€‚æ€»è€—æ—¶: {elapsed_time:.2f} ç§’")
        logger.info(f"ç»Ÿè®¡ï¼šæˆåŠŸæ›´æ–° {successful} ä¸ªæ–‡ä»¶ï¼Œå¤±è´¥/è·³è¿‡ {failed} ä¸ªã€‚")

    finally:
        lock_file_path.unlink(missing_ok=True)
        logger.info("é”æ–‡ä»¶å·²æ¸…é™¤ã€‚")

if __name__ == "__main__":
    main()
