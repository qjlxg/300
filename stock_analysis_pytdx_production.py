# stock_analysis_pytdx_production.py - æœ€ç»ˆ pytdx (é€šè¾¾ä¿¡) ç¨³å®šç‰ˆ (ç§»é™¤ best_ip ä¾èµ–)

import pandas as pd
import pandas_ta as ta
from datetime import datetime, timedelta
import pytz
from concurrent.futures import ThreadPoolExecutor
import time 

# --- æ–°å¢ pytdx ä¾èµ– ---
from pytdx.hq import TdxHq_API
# from pytdx.util import best_ip  # <-- å·²ç§»é™¤ï¼Œä¸å†ä½¿ç”¨
from pytdx.errors import TdxConnectionError
from pytdx.exhq import TdxExHq_API 

# --- é¡¶éƒ¨æ–°å¢å¯¼å…¥ ---
import logging
from pathlib import Path
from tqdm import tqdm
import warnings
warnings.filterwarnings("ignore")
warnings.simplefilter(action='ignore', category=FutureWarning)

# --- å¸¸é‡å’Œé…ç½® ---
shanghai_tz = pytz.timezone('Asia/Shanghai')
OUTPUT_DIR = "index_data" 
DEFAULT_START_DATE = '2000-01-01' # pytdx æ¥å£éœ€è¦ YYYY-MM-DD æ ¼å¼
INDICATOR_LOOKBACK_DAYS = 30 
LOCK_FILE = "stock_analysis.lock" 

MAX_WORKERS = 1 
MAX_RETRIES = 3 # pytdx è¿æ¥é‡è¯•æ¬¡æ•°ï¼Œå°è¯• 3 æ¬¡

# pytdx å‘¨æœŸæ˜ å°„: 9:æ—¥çº¿, 5:å‘¨çº¿, 6:æœˆçº¿, 8:1åˆ†é’Ÿ
TDX_FREQ_MAP = {'D': 9, 'W': 5, 'M': 6}

# --- åŠ¨æ€ IP è·å–å‡½æ•° (å·²ä¿®æ­£ï¼Œä»…ä½¿ç”¨ç¨³å®šåˆ—è¡¨) ---

def get_best_servers(num_servers=5):
    """ç›´æ¥è¿”å›ç¨³å®šçš„ pytdx æœåŠ¡å™¨å¤‡ç”¨åˆ—è¡¨ï¼Œé¿å… best_ip å¸¦æ¥çš„ä¸ç¨³å®šæ€§ã€‚"""
    
    # ç¤¾åŒºæ¨èçš„ç¨³å®šå¤‡ç”¨åˆ—è¡¨ï¼ˆä¼˜å…ˆçº§é«˜ï¼‰
    stable_servers = [
        ('114.80.149.19', 7709),    # åæ³°è¯åˆ¸
        ('114.80.149.22', 7709),    # åæ³°å¤‡ç”¨
        ('114.80.149.84', 7709),    # åæ³°å¤‡ç”¨
        ('114.80.80.222', 7709),    # å›½é‡‘è¯åˆ¸
        ('115.238.56.198', 7709),  # æ–°æ—¶ä»£
        ('119.147.164.60', 7709),  # å¹¿å‘è¯åˆ¸
        ('123.125.108.23', 7709),  # ä¸­é‡‘å…¬å¸
        ('180.153.18.17', 7709),    # æ‹›å•†è¯åˆ¸
        ('121.36.81.195', 7709),    # ç¤¾åŒºæ¨èï¼ˆ2025 æ›´æ–°ï¼‰
        ('124.71.187.122', 7709),  # å¤‡ç”¨
        ('119.147.212.81', 7721),  # é€šç”¨å¤‡ç”¨ç«¯å£
        ('119.147.212.81', 7709),  # é€šç”¨ä¸»ç”¨ç«¯å£
    ]
    
    logger.info("    - ç»•è¿‡ pytdx best_ip è‡ªåŠ¨é€‰æ‹©åŠŸèƒ½ï¼Œä½¿ç”¨ç¡¬ç¼–ç çš„ç¨³å®š IP åˆ—è¡¨ã€‚")
    return stable_servers[:num_servers]

# å®šä¹‰æ‰€æœ‰ä¸»è¦ A è‚¡æŒ‡æ•°åˆ—è¡¨
INDEX_LIST = {
    '000001': {'name': 'ä¸Šè¯æŒ‡æ•°', 'market': 1}, 
    '399001': {'name': 'æ·±è¯æˆæŒ‡', 'market': 0}, 
    '399006': {'name': 'åˆ›ä¸šæ¿æŒ‡', 'market': 0},
    '000016': {'name': 'ä¸Šè¯50', 'market': 1}, 
    '000300': {'name': 'æ²ªæ·±300', 'market': 1}, 
    '000905': {'name': 'ä¸­è¯500', 'market': 1},
    '000852': {'name': 'ä¸­è¯1000', 'market': 1}, 
    '000688': {'name': 'ç§‘åˆ›50', 'market': 1}, 
    '399300': {'name': 'æ²ªæ·±300(æ·±)', 'market': 0},
    '000991': {'name': 'ä¸­è¯å…¨æŒ‡', 'market': 1},
    '000906': {'name': 'ä¸­è¯800', 'market': 1}, 
    '399005': {'name': 'ä¸­å°æ¿æŒ‡', 'market': 0}, 
    '399330': {'name': 'æ·±è¯100', 'market': 0},
    '000010': {'name': 'ä¸Šè¯180', 'market': 1}, 
    '000015': {'name': 'çº¢åˆ©æŒ‡æ•°', 'market': 1},
    '000011': {'name': 'ä¸Šè¯åŸºé‡‘æŒ‡æ•°', 'market': 1}, 
    '399305': {'name': 'æ·±è¯åŸºé‡‘æŒ‡æ•°', 'market': 0}, 
    '399306': {'name': 'æ·±è¯ETFæŒ‡æ•°', 'market': 0},
}

# --- è¡Œä¸šæŒ‡æ•°ä»£ç å’Œå¸‚åœºåˆ¤æ–­é€»è¾‘ (ä¿æŒä¸å˜) ---

SW_INDUSTRY_DICT = {'801010':'å†œæ—ç‰§æ¸”','801020':'é‡‡æ˜','801030':'åŒ–å·¥','801040':'é’¢é“','801050':'æœ‰è‰²é‡‘å±',
                    '801080':'ç”µå­','801110':'å®¶ç”¨ç”µå™¨','801120':'é£Ÿå“é¥®æ–™','801130':'çººç»‡æœè£…','801140':'è½»å·¥åˆ¶é€ ',
                    '801150':'åŒ»è¯ç”Ÿç‰©','801160':'å…¬ç”¨äº‹ä¸š','801170':'äº¤é€šè¿è¾“','801180':'æˆ¿åœ°äº§','801200':'å•†ä¸šè´¸æ˜“',
                    '801210':'ä¼‘é—²æœåŠ¡','801230':'ç»¼åˆ','801710':'å»ºç­‘ææ–™','801720':'å»ºç­‘è£…é¥°','801730':'ç”µæ°”è®¾å¤‡',
                    '801740':'å›½é˜²å†›å·¥','801750':'è®¡ç®—æœº','801760':'ä¼ åª’','801770':'é€šä¿¡','801780':'é“¶è¡Œ','801790':'éé“¶é‡‘è',
                    '801880':'æ±½è½¦','801890':'æœºæ¢°è®¾å¤‡','801060':'å»ºç­‘å»ºæ','801070':'æœºæ¢°è®¾å¤‡','801090':'äº¤è¿è®¾å¤‡',
                    '801190':'é‡‘èæœåŠ¡','801100':'ä¿¡æ¯è®¾å¤‡','801220':'ä¿¡æ¯æœåŠ¡'}

CS_INDUSTRY_DICT = {'CI005001':'çŸ³æ²¹çŸ³åŒ–','CI005002':'ç…¤ç‚­','CI005003':'æœ‰è‰²é‡‘å±','CI005004':'ç”µåŠ›åŠå…¬ç”¨äº‹ä¸š','CI005005':'é’¢é“',
                    'CI005006':'åŸºç¡€åŒ–å·¥','CI005007':'å»ºç­‘','CI005008':'å»ºæ','CI005009':'è½»å·¥åˆ¶é€ ','CI005010':'æœºæ¢°',
                    'CI005011':'ç”µåŠ›è®¾å¤‡','CI005012':'å›½é˜²å†›å·¥','CI005013':'æ±½è½¦','CI005014':'å•†è´¸é›¶å”®','CI005015':'é¤é¥®æ—…æ¸¸',
                    'CI005016':'å®¶ç”µ','CI005017':'çººç»‡æœè£…','CI005018':'åŒ»è¯','CI005019':'é£Ÿå“é¥®æ–™','CI005020':'å†œæ—ç‰§æ¸”',
                    'CI005021':'é“¶è¡Œ','CI005022':'éé“¶è¡Œé‡‘è','CI005023':'æˆ¿åœ°äº§','CI005024':'äº¤é€šè¿è¾“','CI005025':'ç”µå­å…ƒå™¨ä»¶',
                    'CI005026':'é€šä¿¡','CI005027':'è®¡ç®—æœº','CI005028':'ä¼ åª’','CI005029':'ç»¼åˆ'}

WIND_INDUSTRY_DICT = {'882002':'ææ–™', '882001':'èƒ½æº','882003':'å·¥ä¸š','882004':'å¯é€‰æ¶ˆè´¹','882005':'æ—¥å¸¸æ¶ˆè´¹',
                      '882006':'åŒ»ç–—ä¿å¥', '882007':'é‡‘è', '882008':'ä¿¡æ¯æŠ€æœ¯', '882009':'ç”µä¿¡æœåŠ¡',
                      '882010':'å…¬ç”¨äº‹ä¸š', '882011':'æˆ¿åœ°äº§'}

def get_pytdx_market(code):
    """æ ¹æ®æŒ‡æ•°ä»£ç è§„åˆ™åˆ¤æ–­ pytdx æ‰€éœ€çš„å¸‚åœºä»£ç ã€‚"""
    code = str(code)
    # ä¸Šè¯æŒ‡æ•°ä»£ç ï¼š000xxx, 88xxxx, 801xxx, CI005xxx
    if code.startswith('00') or code.startswith('88') or code.startswith('801') or code.startswith('CI005'):
        return 1  # è§†ä¸ºä¸Šè¯/é€šç”¨çš„æŒ‡æ•°å¸‚åœº
    # æ·±è¯æŒ‡æ•°ä»£ç ï¼š399xxx 
    elif code.startswith('399'):
        return 0
    # å…¶ä»–é»˜è®¤è§†ä¸ºä¸Šè¯
    return 1 

def merge_industry_indexes(index_list, industry_dict, prefix=""):
    """å°†è¡Œä¸šå­—å…¸åˆå¹¶åˆ° INDEX_LIST ä¸­ï¼Œå¹¶è‡ªåŠ¨åˆ¤æ–­ market ä»£ç ã€‚"""
    for code, name in industry_dict.items():
        pytdx_code = code.split('.')[0] 
        if pytdx_code not in index_list:
            index_list[pytdx_code] = {
                'name': f'{prefix}{name}',
                'market': get_pytdx_market(pytdx_code)
            }
    return index_list

# åˆå¹¶æ‰€æœ‰è¡Œä¸šæŒ‡æ•°åˆ° INDEX_LIST
INDEX_LIST = merge_industry_indexes(INDEX_LIST, SW_INDUSTRY_DICT, prefix="ç”³ä¸‡ä¸€çº§_")
INDEX_LIST = merge_industry_indexes(INDEX_LIST, CS_INDUSTRY_DICT, prefix="ä¸­ä¿¡ä¸€çº§_")
INDEX_LIST = merge_industry_indexes(INDEX_LIST, WIND_INDUSTRY_DICT, prefix="ä¸‡å¾—ä¸€çº§_")

# --- é…ç½®æ—¥å¿—ç³»ç»Ÿ (ä¿æŒä¸å˜) ---
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)s | %(message)s',
    handlers=[
        logging.FileHandler("stock_analysis.log", encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# --- è¿æ¥å®¢æˆ·ç«¯ (è¿æ¥é‡è¯•) ---

def connect_tdx_api(servers):
    """å°è¯•è¿æ¥é€šè¾¾ä¿¡è¡Œæƒ… API"""
    api = TdxHq_API()
    for ip, port in servers:
        try:
            logger.info(f"    - å°è¯•è¿æ¥ pytdx æœåŠ¡å™¨: {ip}:{port}")
            if api.connect(ip, port):
                logger.info(f"    - è¿æ¥æˆåŠŸ: {ip}:{port}")
                return api
        except TdxConnectionError:
            logger.warning(f"    - è¿æ¥å¤±è´¥: {ip}:{port}")
        except Exception as e:
            logger.error(f"    - è¿æ¥ {ip}:{port} æ—¶å‘ç”Ÿæ„å¤–é”™è¯¯: {e}")
            
    return None

# --- æŒ‡æ ‡è®¡ç®—å‡½æ•° (ä¿æŒä¸å˜) ---

def calculate_full_technical_indicators(df):
    """è®¡ç®—å®Œæ•´çš„æŠ€æœ¯æŒ‡æ ‡é›†ï¼šMA, RSI, KDJ, MACD, BBANDS, ATR, CCI, OBV"""
    if df.empty:
        return df
    
    df = df.set_index('date')
    # ä½¿ç”¨ pandas_ta è®¡ç®—æŒ‡æ ‡
    df.ta.sma(length=5, append=True, col_names=('MA5',))
    df.ta.sma(length=20, append=True, col_names=('MA20',))
    df.ta.rsi(length=14, append=True, col_names=('RSI14',))
    df.ta.stoch(k=9, d=3, smooth_k=3, append=True) 
    df = df.rename(columns={'STOCHk_9_3_3': 'K', 'STOCHd_9_3_3': 'D', 'STOCHj_9_3_3': 'J'})
    df.ta.macd(append=True)
    df = df.rename(columns={'MACD_12_26_9': 'MACD', 'MACDh_12_26_9': 'MACDh', 'MACDs_12_26_9': 'MACDs'})
    df.ta.bbands(length=20, std=2, append=True)
    df = df.rename(columns={
        'BBL_20_2.0': 'BB_lower', 'BBM_20_2.0': 'BB_middle', 'BBU_20_2.0': 'BB_upper',
        'BBB_20_2.0': 'BB_bandwidth', 'BBP_20_2.0': 'BB_percent'
    })
    df.ta.atr(length=14, append=True)
    df = df.rename(columns={'ATRr_14': 'ATR14'})
    df.ta.cci(length=20, append=True)
    df = df.rename(columns={'CCI_20_0.015': 'CCI20'})
    df.ta.obv(append=True)
    
    return df.reset_index()


def aggregate_and_analyze(df_raw_slice, freq, prefix):
    """æŒ‰é¢‘ç‡èšåˆæ•°æ®å¹¶è®¡ç®—æŒ‡æ ‡"""
    if df_raw_slice.empty:
        return pd.DataFrame()
        
    df_raw_slice['turnover_rate'] = float('nan') # å ä½
    
    # å°† index è½¬æ¢ä¸º datetime ç”¨äº resample
    df_raw_slice.index = pd.to_datetime(df_raw_slice.index)
    
    agg_df = df_raw_slice.resample(freq).agg({
        'open': 'first', 'high': 'max', 'low': 'min', 'close': 'last',
        'vol': 'sum', 'turnover_rate': 'mean'
    }).dropna(subset=['close'])
    
    if not agg_df.empty:
        agg_df = agg_df.reset_index().rename(columns={'index': 'date', 'vol': 'volume'})
        agg_df['date'] = agg_df['date'].dt.date # ä¿æŒ date ä¸º date å¯¹è±¡
        agg_df = calculate_full_technical_indicators(agg_df)
        
        cols_to_keep = agg_df.columns.drop(['date', 'open', 'close', 'high', 'low', 'volume', 'turnover_rate'])
        agg_df = agg_df[['date'] + cols_to_keep.tolist()]
        agg_df = agg_df.rename(columns={col: f'{col}_{prefix}' for col in cols_to_keep})
        agg_df.set_index('date', inplace=True)
        
    return agg_df

# --- å¢é‡æ•°æ®è·å–ä¸åˆ†ææ ¸å¿ƒå‡½æ•° (ä½¿ç”¨ pytdx åˆ†é¡µ) ---

def get_full_history_data(api, market, code, freq):
    """ä½¿ç”¨ pytdx åˆ†é¡µè·å–å®Œæ•´çš„å†å² K çº¿æ•°æ®ã€‚"""
    all_data = []
    
    # ä»æœ€æ–°çš„æ•°æ®å¼€å§‹å¾€å‰åˆ†é¡µè·å– (å®‰å…¨é™åˆ¶ 50000 æ¡)
    for start in range(0, 50000, 800): 
        try:
            data = api.get_security_bars(freq, market, code, start, 800)
            
            if not data:
                break
            
            df = api.to_df(data)
            
            if df.empty:
                break
            
            all_data.append(df)
            
            if len(df) < 800:
                break
            
        except Exception as e:
            logger.error(f"    - pytdx åˆ†é¡µè·å– {code} å¤±è´¥ (Start={start})ã€‚é”™è¯¯: {e}")
            break 
            
    if all_data:
        df_combined = pd.concat(all_data, ignore_index=True)
        df_combined.drop_duplicates(subset=['datetime'], keep='first', inplace=True)
        df_combined.sort_values(by='datetime', inplace=True)
        
        df_combined['date'] = pd.to_datetime(df_combined['datetime']).dt.date
        df_combined.set_index('date', inplace=True)
        
        return df_combined
    return pd.DataFrame()


def get_and_analyze_data_slice(api, market, code, start_date):
    """è·å–æ•°æ®åˆ‡ç‰‡ï¼ŒåŒ…æ‹¬å…¨é‡è·å–ã€æœ¬åœ°ç­›é€‰å’ŒæŒ‡æ ‡è®¡ç®—ã€‚"""
    logger.info(f"    - æ­£åœ¨è·å– {code} (pytdx æ¥å£) å…¨é‡æ•°æ®...")

    try:
        # 1. å…¨é‡è·å–æ•°æ®
        df_full = get_full_history_data(api, market, code, TDX_FREQ_MAP['D'])

        if df_full.empty:
            logger.warning(f"    - {code} æœªè·å–åˆ°æ•°æ®ã€‚")
            return None
            
        # 2. æœ¬åœ°ç­›é€‰ï¼ˆè·å–å¢é‡/é‡å åˆ‡ç‰‡ï¼‰
        start_dt = datetime.strptime(start_date, '%Y-%m-%d').date()
        df_raw = df_full[df_full.index >= start_dt].copy()

        if df_raw.empty:
            logger.warning(f"    - {code} ç­›é€‰ååˆ‡ç‰‡ä¸ºç©ºã€‚")
            return None
            
        # 3. pytdx æ•°æ®æ¸…æ´—å’Œé‡å‘½å
        df_raw.rename(columns={'vol': 'volume'}, inplace=True)
        
        # 4. æŒ‡æ ‡è®¡ç®—
        df_raw_processed = df_raw[['open', 'close', 'high', 'low', 'volume']].copy()
        df_raw_processed = df_raw_processed.reset_index()

        df_daily = calculate_full_technical_indicators(df_raw_processed.copy())
        
        # 5. å‘¨/æœˆ/å¹´æŒ‡æ ‡èšåˆè®¡ç®—
        df_raw.reset_index(inplace=True)
        df_raw['turnover_rate'] = float('nan') 
        df_raw.set_index('date', inplace=True)
        
        daily_cols = df_daily.columns.drop(['date', 'open', 'close', 'high', 'low', 'volume', 'turnover_rate'])
        df_daily = df_daily.rename(columns={col: f'{col}_D' for col in daily_cols})
        df_daily.set_index('date', inplace=True)
        
        # èšåˆå’Œåˆå¹¶
        df_weekly = aggregate_and_analyze(df_raw, 'W', 'W')
        df_monthly = aggregate_and_analyze(df_raw, 'M', 'M')
        df_yearly = aggregate_and_analyze(df_raw, 'Y', 'Y')

        results = df_daily.copy()
        results = results.join(df_weekly, how='left').join(df_monthly, how='left').join(df_yearly, how='left')
        results.index.name = 'date'
        
        logger.info(f"    - {code} æˆåŠŸåˆ†æ {len(results)} è¡Œæ•°æ®åˆ‡ç‰‡ã€‚")
        return results.sort_index()

    except Exception as e:
        logger.error(f"    - é”™è¯¯ï¼šå¤„ç†æŒ‡æ•° {code} å¤±è´¥ã€‚æœ€ç»ˆé”™è¯¯: {e}")
        return None

# --- å•ä¸ªæŒ‡æ•°å¤„ç†å’Œä¿å­˜å‡½æ•° (é€‚é… pytdx) ---

def process_single_index(api, code_map):
    """å¤„ç†å•ä¸ªæŒ‡æ•°ï¼Œå®ç°å¢é‡ä¸‹è½½ã€è®¡ç®—å’Œè¦†ç›–ä¿å­˜"""
    code = code_map['code']
    name = code_map['name']
    market = code_map['market']
    
    logger.info(f"-> æ­£åœ¨å¤„ç†æŒ‡æ•°: {code} ({name})")
    
    file_name = f"{code.replace('.', '_')}.csv"
    output_path = Path(OUTPUT_DIR) / file_name
    
    start_date_to_request = DEFAULT_START_DATE
    df_old = pd.DataFrame()
    
    # 1. ç¡®å®šæœ¬æ¬¡ä¸‹è½½çš„èµ·å§‹æ—¥æœŸ 
    if output_path.exists():
        try:
            df_old = pd.read_csv(output_path, index_col='date', parse_dates=True)
            if not df_old.empty:
                latest_date_in_repo = df_old.index.max()
                
                # å¾€å‰æ¨ INDICATOR_LOOKBACK_DAYS å¤©ï¼Œç¡®ä¿æœ‰è¶³å¤Ÿæ•°æ®è®¡ç®—æŒ‡æ ‡
                start_date_for_calc = latest_date_in_repo - timedelta(days=INDICATOR_LOOKBACK_DAYS)
                start_date_to_request = start_date_for_calc.strftime('%Y-%m-%d')
                
                if start_date_for_calc.strftime('%Y-%m-%d') < DEFAULT_START_DATE:
                    start_date_to_request = DEFAULT_START_DATE
                
                logger.info(f"    - æ£€æµ‹åˆ°æ—§æ•°æ®ï¼Œæœ€æ–°æ—¥æœŸä¸º {latest_date_in_repo.strftime('%Y-%m-%d')}ã€‚æœ¬åœ°ç­›é€‰ä» {start_date_to_request} å¼€å§‹çš„åˆ‡ç‰‡ï¼ˆå«é‡å ï¼‰ã€‚")
            else:
                logger.warning(f"    - æ—§æ–‡ä»¶ {output_path.name} ä¸ºç©ºï¼Œå°†å…¨é‡ä¸‹è½½ã€‚")
        except Exception as e:
            logger.error(f"    - è­¦å‘Šï¼šè¯»å–æ—§æ–‡ä»¶ {output_path.name} å¤±è´¥ ({e})ï¼Œå°†å…¨é‡ä¸‹è½½ã€‚")
            
    else:
        logger.info(f"    - æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°†å…¨é‡ä¸‹è½½ã€‚")


    # 2. è·å–æœ€æ–°æ•°æ®å’ŒæŒ‡æ ‡ 
    df_new_analyzed = get_and_analyze_data_slice(api, market, code, start_date_to_request)
    
    if df_new_analyzed is None:
        today = datetime.now(shanghai_tz).date()
        if not df_old.empty and df_old.index.max().date() == today:
            logger.info(f"    - {code} æ•°æ®å·²æ˜¯ä»Šå¤©æœ€æ–°ï¼Œè·³è¿‡ä¿å­˜ã€‚")
        else:
            logger.warning(f"    - {code} æœªè·å–åˆ°æ–°æ•°æ®ï¼Œä¿æŒåŸæ–‡ä»¶ã€‚")
        return False

    # 3. æ•´åˆæ–°æ—§æ•°æ® 
    if not df_old.empty:
        old_data_to_keep = df_old[df_old.index < df_new_analyzed.index.min()]
    else:
        old_data_to_keep = pd.DataFrame()


    df_combined = pd.concat([old_data_to_keep, df_new_analyzed])
    results_to_save = df_combined[~df_combined.index.duplicated(keep='last')]
    results_to_save = results_to_save.sort_index()

    logger.info(f"    - âœ… {code} æˆåŠŸæ›´æ–°ã€‚æ€»è¡Œæ•°: {len(results_to_save)}")
    
    # 4. ä¿å­˜åˆ° CSV 
    results_to_save.to_csv(output_path, encoding='utf-8')
    return True

# --- ä¸»æ‰§è¡Œé€»è¾‘ ---
def main():
    start_time = time.time()
    output_path = Path(OUTPUT_DIR)
    
    # 1. æ£€æŸ¥è¿è¡Œé”
    lock_file_path = Path(LOCK_FILE)
    if lock_file_path.exists():
        logger.warning("æ£€æµ‹åˆ°é”æ–‡ä»¶ï¼Œè„šæœ¬å¯èƒ½æ­£åœ¨è¿è¡Œæˆ–ä¸Šæ¬¡å¼‚å¸¸é€€å‡ºã€‚ç»ˆæ­¢æœ¬æ¬¡è¿è¡Œã€‚")
        return
    lock_file_path.touch() 
    
    # 2. è¿æ¥ pytdx APIï¼ˆåŠ¨æ€æœåŠ¡å™¨ï¼‰
    tdx_api = None
    servers = get_best_servers(5)  # è·å– 5 ä¸ªç¨³å®šæœåŠ¡å™¨
    logger.info(f"    - ä½¿ç”¨æœåŠ¡å™¨åˆ—è¡¨: {servers}")
    
    for attempt in range(MAX_RETRIES):
        tdx_api = connect_tdx_api(servers)  # ä¼ å…¥åŠ¨æ€åˆ—è¡¨
        if tdx_api:
            break
        if attempt < MAX_RETRIES - 1:
            time.sleep(5)
    
    if not tdx_api:
        logger.error("âŒ æ— æ³•è¿æ¥åˆ°ä»»ä½• pytdx æœåŠ¡å™¨ï¼Œè„šæœ¬ç»ˆæ­¢ã€‚")
        lock_file_path.unlink(missing_ok=True)
        return
        
    try:
        # 3. åˆå§‹åŒ–ç›®å½•å’Œæ—¥å¿—
        output_path.mkdir(exist_ok=True) 
        logger.info("â€”" * 50)
        logger.info("ğŸš€ è„šæœ¬å¼€å§‹è¿è¡Œ (ä½¿ç”¨ pytdx)")
        logger.info(f"ç»“æœå°†ä¿å­˜åˆ°ä¸“ç”¨ç›®å½•: {output_path.resolve()}")
        logger.info(f"å‡†å¤‡ä¸²è¡Œå¤„ç† {len(INDEX_LIST)} ä¸ªæŒ‡æ•°...")

        successful = 0
        failed = 0
        
        # 4. è½¬æ¢ INDEX_LIST æ ¼å¼ä»¥æ–¹ä¾¿å¤„ç†
        jobs = [{'code': code, **data} for code, data in INDEX_LIST.items()]
        
        # 5. ä½¿ç”¨ ThreadPoolExecutor è¿›è¡Œä¸²è¡Œå¤„ç† (MAX_WORKERS = 1)
        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            # æäº¤ä»»åŠ¡ï¼Œå°† API å®¢æˆ·ç«¯ä½œä¸ºå‚æ•°ä¼ å…¥
            futures = {
                executor.submit(process_single_index, tdx_api, job): job
                for job in jobs
            }
            
            # ä½¿ç”¨ tqdm åŒ…è£… futures å¾ªç¯
            for future in tqdm(futures, desc="å¤„ç†æŒ‡æ•°", unit="ä¸ª", ncols=100, leave=True):
                job = futures[future]
                try:
                    if future.result():
                        successful += 1
                    else:
                        failed += 1
                except Exception as e:
                    logger.error(f"å¤„ç† {job['code']} ({job['name']}) æ—¶å‘ç”Ÿæœªæ•è·å¼‚å¸¸: {e}")
                    failed += 1
        
        end_time = time.time()
        elapsed_time = end_time - start_time
        
        # 6. æœ€ç»ˆç»Ÿè®¡å’Œè¾“å‡º
        logger.info("â€”" * 50)
        logger.info(f"âœ… æ‰€æœ‰æŒ‡æ•°æ•°æ®å¤„ç†å®Œæˆã€‚æ€»è€—æ—¶: {elapsed_time:.2f} ç§’")
        logger.info(f"ç»Ÿè®¡ï¼šæˆåŠŸæ›´æ–° {successful} ä¸ªæ–‡ä»¶ï¼Œå¤±è´¥/è·³è¿‡ {failed} ä¸ªã€‚")

    finally:
        # 7. ç§»é™¤é”æ–‡ä»¶å¹¶æ–­å¼€è¿æ¥ (å¢å¼ºé”™è¯¯å¤„ç†)
        if tdx_api: # ç¡®ä¿ tdx_api å®ä¾‹å­˜åœ¨
            try:
                tdx_api.close()
            except Exception as e:
                logger.warning(f"å…³é—­ pytdx è¿æ¥æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                
        lock_file_path.unlink(missing_ok=True)
        logger.info("pytdx è¿æ¥å·²å…³é—­ï¼Œé”æ–‡ä»¶å·²æ¸…é™¤ã€‚")

if __name__ == "__main__":
    main()
