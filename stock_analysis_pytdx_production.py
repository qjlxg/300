# stock_analysis_akshare_production_V11_FAILOVER.py - V11 æ•…éšœåˆ‡æ¢ç‰ˆ (å¢åŠ  ak.index_zh_a_daily å†—ä½™æ•°æ®æº)

import pandas as pd
import pandas_ta as ta
from datetime import datetime, timedelta
import pytz
from concurrent.futures import ThreadPoolExecutor 
import time
import akshare as ak
import logging
from pathlib import Path
from tqdm import tqdm
import warnings
warnings.filterwarnings("ignore")
warnings.simplefilter(action='ignore', category=FutureWarning)


# --- AkShare å…¨å±€é…ç½® ---
try:
    ak.set_time_out(30)
except Exception as e:
    print(f"è­¦å‘Šï¼šè®¾ç½® AkShare å…¨å±€è¶…æ—¶å¤±è´¥ï¼š{e}")


# --- å¸¸é‡å’Œé…ç½® ---
shanghai_tz = pytz.timezone('Asia/Shanghai')
OUTPUT_DIR = "index_data"
DEFAULT_START_DATE = '2000-01-01'
INDICATOR_LOOKBACK_DAYS = 30
LOCK_FILE = "stock_analysis.lock"

MAX_WORKERS = 1
MAX_RETRIES = 0 # é»˜è®¤æ¥å£çš„æœ€å¤§é‡è¯•æ¬¡æ•°
BASE_DELAY = 10 

# --- æŒ‡æ•°åˆ—è¡¨åŠä»£ç ç»“æ„ (ä¿æŒä¸å˜) ---
INDEX_LIST = {
    '000001': {'name': 'ä¸Šè¯æŒ‡æ•°', 'market': 1},
    '399001': {'name': 'æ·±è¯æˆæŒ‡', 'market': 0},
    '399006': {'name': 'åˆ›ä¸šæ¿æŒ‡', 'market': 0},
    '000016': {'name': 'ä¸Šè¯50', 'market': 1},
    '000300': {'name': 'æ²ªæ·±300', 'market': 1},
    '000905': {'name': 'ä¸­è¯500', 'market': 1},
    '000852': {'name': 'ä¸­è¯1000', 'market': 1},
    '000688': {'name': 'ç§‘åˆ›50', 'market': 1},
    '399300': {'name': 'æ²ªæ·±300(æ·±)', 'market': 0},
    '000991': {'name': 'ä¸­è¯å…¨æŒ‡', 'market': 1},
    '000906': {'name': 'ä¸­è¯800', 'market': 1},
    '399005': {'name': 'ä¸­å°æ¿æŒ‡', 'market': 0},
    '399330': {'name': 'æ·±è¯100', 'market': 0},
    '000010': {'name': 'ä¸Šè¯180', 'market': 1},
    '000015': {'name': 'çº¢åˆ©æŒ‡æ•°', 'market': 1},
    '000011': {'name': 'ä¸Šè¯åŸºé‡‘æŒ‡æ•°', 'market': 1},
    '399305': {'name': 'æ·±è¯åŸºé‡‘æŒ‡æ•°', 'market': 0},
    '399306': {'name': 'æ·±è¯ETFæŒ‡æ•°', 'market': 0},
}
SW_INDUSTRY_DICT = {'801010':'å†œæ—ç‰§æ¸”','801020':'é‡‡æ˜','801030':'åŒ–å·¥','801040':'é’¢é“','801050':'æœ‰è‰²é‡‘å±','801080':'ç”µå­','801110':'å®¶ç”¨ç”µå™¨','801120':'é£Ÿå“é¥®æ–™','801130':'çººç»‡æœè£…','801140':'è½»å·¥åˆ¶é€ ','801150':'åŒ»è¯ç”Ÿç‰©','801160':'å…¬ç”¨äº‹ä¸š','801170':'äº¤é€šè¿è¾“','801180':'æˆ¿åœ°äº§','801200':'å•†ä¸šè´¸æ˜“','801210':'ä¼‘é—²æœåŠ¡','801230':'ç»¼åˆ','801710':'å»ºç­‘ææ–™','801720':'å»ºç­‘è£…é¥°','801730':'ç”µæ°”è®¾å¤‡','801740':'å›½é˜²å†›å·¥','801750':'è®¡ç®—æœº','801760':'ä¼ åª’','801770':'é€šä¿¡','801780':'é“¶è¡Œ','801790':'éé“¶é‡‘è','801880':'æ±½è½¦','801890':'æœºæ¢°è®¾å¤‡','801060':'å»ºç­‘å»ºæ','801070':'æœºæ¢°è®¾å¤‡','801090':'äº¤è¿è®¾å¤‡','801190':'é‡‘èæœåŠ¡','801100':'ä¿¡æ¯è®¾å¤‡','801220':'ä¿¡æ¯æœåŠ¡'}
CS_INDUSTRY_DICT = {}
WIND_INDUSTRY_DICT = {}

def get_pytdx_market(code):
    code = str(code)
    if code.startswith('00') or code.startswith('88') or code.startswith('801') or code.startswith('CI005'):
        return 1
    elif code.startswith('399'):
        return 0
    return 1

def merge_industry_indexes(index_list, industry_dict, prefix=""):
    for code, name in industry_dict.items():
        pytdx_code = code.split('.')[0]
        if pytdx_code not in index_list:
            index_list[pytdx_code] = {
                'name': f'{prefix}{name}',
                'market': get_pytdx_market(pytdx_code)
            }
    return index_list

INDEX_LIST = merge_industry_indexes(INDEX_LIST, SW_INDUSTRY_DICT, prefix="ç”³ä¸‡ä¸€çº§_")
INDEX_LIST = merge_industry_indexes(INDEX_LIST, CS_INDUSTRY_DICT, prefix="ä¸­ä¿¡ä¸€çº§_")
INDEX_LIST = merge_industry_indexes(INDEX_LIST, WIND_INDUSTRY_DICT, prefix="ä¸‡å¾—ä¸€çº§_")

# --- æ–°å¢ï¼šè·å–æ–°æµªæ¥å£æ‰€éœ€çš„å¸¦å¸‚åœºå‰ç¼€çš„ä»£ç  ---
def get_index_symbol_with_prefix(code):
    """æ ¹æ®ä»£ç åˆ¤æ–­å¸‚åœºï¼Œè¿”å›å¸¦å‰ç¼€çš„æŒ‡æ•°ä»£ç ï¼Œç”¨äºæ–°æµª/è…¾è®¯æ¥å£"""
    code = str(code)
    # ä¸Šäº¤æ‰€æŒ‡æ•° (000xxx å¼€å¤´)
    if code.startswith('0'):
        return f'sh{code}'
    # æ·±äº¤æ‰€æŒ‡æ•° (399xxx å¼€å¤´)
    elif code.startswith('3') or code.startswith('801'):
        return f'sz{code}'
    return code
# --- æ—¥å¿—ç³»ç»Ÿ (ä¿æŒä¸å˜) ---
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)s | %(message)s',
    handlers=[
        logging.FileHandler("stock_analysis.log", encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


# --- æŒ‡æ ‡è®¡ç®—å‡½æ•° (ä¿æŒä¸å˜) ---
def calculate_full_technical_indicators(df):
    if df.empty: return df
    df['date'] = pd.to_datetime(df['date'])
    df = df.set_index('date')
    price_cols = ['open', 'close', 'high', 'low', 'volume']
    for col in price_cols:
        df[col] = pd.to_numeric(df[col], errors='coerce')
    df.ta.sma(length=5, append=True, col_names=('MA5',))
    df.ta.sma(length=20, append=True, col_names=('MA20',))
    df.ta.rsi(length=14, append=True, col_names=('RSI14',))
    df.ta.stoch(k=9, d=3, smooth_k=3, append=True); df = df.rename(columns={'STOCHk_9_3_3': 'K', 'STOCHd_9_3_3': 'D', 'STOCHj_9_3_3': 'J'})
    df.ta.macd(append=True); df = df.rename(columns={'MACD_12_26_9': 'MACD', 'MACDh_12_26_9': 'MACDh', 'MACDs_12_26_9': 'MACDs'})
    df.ta.bbands(length=20, std=2, append=True); df = df.rename(columns={'BBL_20_2.0': 'BB_lower', 'BBM_20_2.0': 'BB_middle', 'BBU_20_2.0': 'BB_upper', 'BBB_20_2.0': 'BB_bandwidth', 'BBP_20_2.0': 'BB_percent'})
    df.ta.atr(length=14, append=True); df = df.rename(columns={'ATRr_14': 'ATR14'})
    df.ta.cci(length=20, append=True); df = df.rename(columns={'CCI_20_0.015': 'CCI20'})
    df.ta.obv(append=True)
    return df.reset_index()

def aggregate_and_analyze(df_raw_slice, freq, prefix):
    if df_raw_slice.empty: return pd.DataFrame()
    df_raw_slice['turnover_rate'] = float('nan')
    df_raw_slice.index = pd.to_datetime(df_raw_slice.index)
    agg_df = df_raw_slice.resample(freq).agg({
        'open': 'first', 'high': 'max', 'low': 'min', 'close': 'last',
        'volume': 'sum', 'turnover_rate': 'mean'
    }).dropna(subset=['close'])
    if not agg_df.empty:
        agg_df = agg_df.reset_index().rename(columns={'index': 'date'})
        agg_df['date'] = agg_df['date'].dt.date
        agg_df = calculate_full_technical_indicators(agg_df)
        cols_to_keep = agg_df.columns.drop(['date', 'open', 'close', 'high', 'low', 'volume', 'turnover_rate'])
        agg_df = agg_df.rename(columns={col: f'{col}_{prefix}' for col in cols_to_keep})
        agg_df.set_index('date', inplace=True)
    return agg_df

# --- å¢é‡æ•°æ®è·å–ä¸åˆ†ææ ¸å¿ƒå‡½æ•° (æ–°å¢ Failover é€»è¾‘) ---

def get_full_history_data(code, start_date_str):
    """
    é¦–å…ˆå°è¯•é»˜è®¤çš„ ak.index_zh_a_hist æ¥å£ï¼Œå¤±è´¥åæ•…éšœåˆ‡æ¢åˆ° ak.index_zh_a_daily (æ–°æµª)ã€‚
    """
    if code.startswith('801'):
        logger.warning(f"    - è­¦å‘Šï¼šè¡Œä¸šæŒ‡æ•° {code} æ¥å£å¤æ‚æˆ–ä¸ç¨³å®šï¼Œè·³è¿‡ã€‚")
        return pd.DataFrame()

    start_date_ak = start_date_str.replace('-', '')
    
    # --- å°è¯•ç­–ç•¥ 1: AkShare é»˜è®¤æŒ‡æ•°æ¥å£ (ak.index_zh_a_hist) ---
    logger.info(f"    - å°è¯• 1/2: é€šè¿‡ AkShare é»˜è®¤æ¥å£è·å– {code}...")
    for attempt in range(MAX_RETRIES):
        try:
            df = ak.index_zh_a_hist(
                symbol=code, 
                period="daily", 
                start_date=start_date_ak,
                end_date=datetime.now().strftime('%Y%m%d')
            )
            if df is None or df.empty:
                 raise ValueError("é»˜è®¤æ¥å£è¿”å› None æˆ–ç©ºæ•°æ®ã€‚")
            
            # æ¸…æ´—é»˜è®¤æ¥å£è¿”å›çš„åˆ—å
            df.rename(columns={
                'æ—¥æœŸ': 'date', 'å¼€ç›˜': 'open', 'æ”¶ç›˜': 'close', 
                'æœ€é«˜': 'high', 'æœ€ä½': 'low', 'æˆäº¤é‡': 'volume'
            }, inplace=True)
            df['turnover_rate'] = float('nan') # å…¼å®¹åç»­å¤„ç†
            
            logger.info(f"    - âœ… {code} é»˜è®¤æ¥å£è·å–æˆåŠŸ (å°è¯•æ¬¡æ•°: {attempt + 1})ã€‚")
            break # æˆåŠŸåˆ™è·³å‡ºé‡è¯•å¾ªç¯
        
        except Exception as e:
            logger.warning(f"    - é»˜è®¤æ¥å£è·å– {code} å¤±è´¥ (å°è¯• {attempt + 1}/{MAX_RETRIES})ã€‚é”™è¯¯: {e}")
            if attempt < MAX_RETRIES - 1:
                wait_time = BASE_DELAY + attempt * 2
                logger.info(f"    - æ­£åœ¨ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                time.sleep(wait_time)
            else:
                logger.warning(f"    - é»˜è®¤æ¥å£æœ€ç»ˆå¤±è´¥ï¼Œå‡†å¤‡åˆ‡æ¢åˆ°å¤‡ç”¨æ¥å£ã€‚")
                df = None
    
    if df is not None and not df.empty:
        # å¦‚æœé»˜è®¤æ¥å£æˆåŠŸï¼Œåˆ™ç›´æ¥è¿›å…¥æ•°æ®å¤„ç†
        pass
    else:
        # --- å°è¯•ç­–ç•¥ 2: æ•…éšœåˆ‡æ¢åˆ°æ–°æµªæŒ‡æ•°æ¥å£ (ak.index_zh_a_daily) ---
        sina_code = get_index_symbol_with_prefix(code)
        logger.warning(f"    - å°è¯• 2/2: åˆ‡æ¢åˆ°æ–°æµªæŒ‡æ•°æ¥å£è·å– {code} (ä»£ç : {sina_code})...")
        
        for attempt in range(MAX_RETRIES):
            try:
                # æ–°æµªæ¥å£å®¹æ˜“å° IPï¼Œå‡å°‘é‡è¯•ï¼Œä½†å¢åŠ å»¶è¿Ÿ
                df = ak.index_zh_a_daily(
                    symbol=sina_code, 
                    start_date=start_date_ak,
                    end_date=datetime.now().strftime('%Y%m%d'),
                    adjust="" # ä¸è¿›è¡Œå¤æƒï¼Œä¿æŒæ•°æ®ä¸€è‡´
                )

                if df is None or df.empty:
                    raise ValueError("æ–°æµªæ¥å£è¿”å› None æˆ–ç©ºæ•°æ®ã€‚")

                # æ–°æµªæ¥å£è¿”å›çš„åˆ—åæ˜¯è‹±æ–‡å°å†™
                df.rename(columns={
                    'date': 'date', 'open': 'open', 'close': 'close', 
                    'high': 'high', 'low': 'low', 'volume': 'volume'
                }, inplace=True)
                df['turnover_rate'] = float('nan') # å…¼å®¹åç»­å¤„ç†
                
                logger.info(f"    - âœ… {code} å¤‡ç”¨æ¥å£è·å–æˆåŠŸ (å°è¯•æ¬¡æ•°: {attempt + 1})ã€‚")
                break
                
            except Exception as e:
                logger.warning(f"    - å¤‡ç”¨æ¥å£è·å– {code} å¤±è´¥ (å°è¯• {attempt + 1}/{MAX_RETRIES})ã€‚é”™è¯¯: {e}")
                if attempt < MAX_RETRIES - 1:
                    # å¤‡ç”¨æ¥å£ä½¿ç”¨æ›´é•¿çš„ç­‰å¾…æ—¶é—´
                    wait_time = BASE_DELAY * 2 + attempt * 5
                    logger.info(f"    - æ­£åœ¨ç­‰å¾… {wait_time} ç§’åé‡è¯•å¤‡ç”¨æ¥å£...")
                    time.sleep(wait_time)
                else:
                    logger.error(f"    - {code} æ‰€æœ‰æ•°æ®æºæœ€ç»ˆå¤±è´¥ï¼Œæ”¾å¼ƒã€‚")
                    return pd.DataFrame()
    
    # --- ç»Ÿä¸€æ•°æ®å¤„ç†æµç¨‹ ---
    if df is not None and not df.empty:
        required_cols = ['date', 'open', 'close', 'high', 'low', 'volume']
        df = df[[c for c in required_cols if c in df.columns]].copy()
        df['date'] = pd.to_datetime(df['date'])
        df.set_index('date', inplace=True)
        for col in ['open', 'close', 'high', 'low', 'volume']:
             df[col] = pd.to_numeric(df[col], errors='coerce')
        df.dropna(subset=['close'], inplace=True)
        df.index = df.index.date
        df.sort_index(inplace=True)
        return df
    
    return pd.DataFrame() # ç¡®ä¿æœ€ç»ˆè¿”å›ç©º DataFrame

# --- å‰©ä½™å‡½æ•° (ä¿æŒä¸å˜) ---
def get_and_analyze_data_slice(code, start_date):
    try:
        df_full = get_full_history_data(code, start_date)
        if df_full.empty:
            logger.warning(f"    - {code} æœªè·å–åˆ°æœ‰æ•ˆæ•°æ®ã€‚")
            return None
        df_raw = df_full.copy()
        df_raw_processed = df_raw.reset_index().rename(columns={'index': 'date'})
        df_raw_processed['date'] = pd.to_datetime(df_raw_processed['date'])
        df_daily = calculate_full_technical_indicators(df_raw_processed.copy())
        df_raw.index = pd.to_datetime(df_raw.index)
        daily_cols = df_daily.columns.drop(['date', 'open', 'close', 'high', 'low', 'volume'])
        df_daily = df_daily.rename(columns={col: f'{col}_D' for col in daily_cols})
        df_daily.set_index('date', inplace=True)
        df_weekly = aggregate_and_analyze(df_raw, 'W', 'W')
        df_monthly = aggregate_and_analyze(df_raw, 'M', 'M')
        df_yearly = aggregate_and_analyze(df_raw, 'Y', 'Y')
        results = df_daily.copy()
        results = results.join(df_weekly, how='left').join(df_monthly, how='left').join(df_yearly, how='left')
        results.index.name = 'date'
        logger.info(f"    - {code} æˆåŠŸåˆ†æ {len(results)} è¡Œæ•°æ®åˆ‡ç‰‡ã€‚")
        return results.sort_index()
    except Exception as e:
        logger.error(f"    - é”™è¯¯ï¼šå¤„ç†æŒ‡æ•° {code} å¤±è´¥ã€‚æœ€ç»ˆé”™è¯¯: {e}")
        return None

def process_single_index(code_map):
    code = code_map['code']
    name = code_map['name']
    logger.info(f"-> æ­£åœ¨å¤„ç†æŒ‡æ•°: {code} ({name})")
    file_name = f"{code.replace('.', '_')}.csv"
    output_path = Path(OUTPUT_DIR) / file_name
    start_date_to_request = DEFAULT_START_DATE
    df_old = pd.DataFrame()
    if output_path.exists():
        try:
            df_old = pd.read_csv(output_path, index_col='date', parse_dates=True)
            if not df_old.empty:
                latest_date_in_repo = df_old.index.max()
                start_date_for_calc = latest_date_in_repo - timedelta(days=INDICATOR_LOOKBACK_DAYS)
                start_date_to_request = start_date_for_calc.strftime('%Y-%m-%d')
                if start_date_for_calc.strftime('%Y-%m-%d') < DEFAULT_START_DATE:
                    start_date_to_request = DEFAULT_START_DATE
                logger.info(f"    - æ£€æµ‹åˆ°æ—§æ•°æ®ï¼Œæœ€æ–°æ—¥æœŸä¸º {latest_date_in_repo.strftime('%Y-%m-%d')}ã€‚API è¯·æ±‚ä» {start_date_to_request} å¼€å§‹çš„åˆ‡ç‰‡ï¼ˆå«é‡å ï¼‰ã€‚")
            else:
                logger.warning(f"    - æ—§æ–‡ä»¶ {output_path.name} ä¸ºç©ºï¼Œå°†å…¨é‡ä¸‹è½½ã€‚")
        except Exception as e:
            logger.error(f"    - è­¦å‘Šï¼šè¯»å–æ—§æ–‡ä»¶ {output_path.name} å¤±è´¥ ({e})ï¼Œå°†å…¨é‡ä¸‹è½½ã€‚")
    else:
        logger.info(f"    - æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°†å…¨é‡ä¸‹è½½ã€‚")
    df_new_analyzed = get_and_analyze_data_slice(code, start_date_to_request)
    if df_new_analyzed is None:
        is_today_updated = False
        if not df_old.empty and pd.api.types.is_datetime64_any_dtype(df_old.index):
             today = datetime.now(shanghai_tz).date()
             is_today_updated = df_old.index.max().date() == today
        if is_today_updated:
            logger.info(f"    - {code} æ•°æ®å·²æ˜¯ä»Šå¤©æœ€æ–°ï¼Œè·³è¿‡ä¿å­˜ã€‚")
        else:
            logger.warning(f"    - {code} æœªè·å–åˆ°æ–°æ•°æ®ï¼Œä¿æŒåŸæ–‡ä»¶ã€‚")
        return False
    if not df_old.empty:
        df_old.index = df_old.index.date
        df_new_analyzed.index = pd.to_datetime(df_new_analyzed.index)
        old_data_to_keep = df_old[df_old.index < df_new_analyzed.index.min().date()]
    else:
        old_data_to_keep = pd.DataFrame()
    df_new_analyzed.index = pd.to_datetime(df_new_analyzed.index)
    old_data_to_keep.index = pd.to_datetime(old_data_to_keep.index)
    df_combined = pd.concat([old_data_to_keep, df_new_analyzed])
    results_to_save = df_combined[~df_combined.index.duplicated(keep='last')]
    results_to_save = results_to_save.sort_index()
    logger.info(f"    - âœ… {code} æˆåŠŸæ›´æ–°ã€‚æ€»è¡Œæ•°: {len(results_to_save)}")
    results_to_save.to_csv(output_path, encoding='utf-8')
    return True

def main():
    start_time = time.time()
    output_path = Path(OUTPUT_DIR)
    lock_file_path = Path(LOCK_FILE)
    if lock_file_path.exists():
        logger.warning("æ£€æµ‹åˆ°é”æ–‡ä»¶ï¼Œè„šæœ¬å¯èƒ½æ­£åœ¨è¿è¡Œæˆ–ä¸Šæ¬¡å¼‚å¸¸é€€å‡ºã€‚ç»ˆæ­¢æœ¬æ¬¡è¿è¡Œã€‚")
        return
    lock_file_path.touch()
    logger.info("â€”" * 50)
    logger.info("ğŸš€ è„šæœ¬å¼€å§‹è¿è¡Œ (ä½¿ç”¨ AkShare V11 - FAILOVER æ¨¡å¼)")
    try:
        output_path.mkdir(exist_ok=True)
        logger.info(f"ç»“æœå°†ä¿å­˜åˆ°ä¸“ç”¨ç›®å½•: {output_path.resolve()}")
        logger.info(f"å‡†å¤‡ä¸²è¡Œå¤„ç† {len(INDEX_LIST)} ä¸ªæŒ‡æ•°...")
        successful = 0
        failed = 0
        jobs = [{'code': code, **data} for code, data in INDEX_LIST.items()]
        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            futures = {executor.submit(process_single_index, job): job for job in jobs}
            for future in tqdm(futures, desc="å¤„ç†æŒ‡æ•°", unit="ä¸ª", ncols=100, leave=True):
                job = futures[future]
                try:
                    if future.result():
                        successful += 1
                    else:
                        failed += 1
                except Exception as e:
                    logger.error(f"å¤„ç† {job['code']} ({job['name']}) æ—¶å‘ç”Ÿæœªæ•è·å¼‚å¸¸: {e}")
                    failed += 1
        end_time = time.time()
        elapsed_time = end_time - start_time
        logger.info("â€”" * 50)
        logger.info(f"âœ… æ‰€æœ‰æŒ‡æ•°æ•°æ®å¤„ç†å®Œæˆã€‚æ€»è€—æ—¶: {elapsed_time:.2f} ç§’")
        logger.info(f"ç»Ÿè®¡ï¼šæˆåŠŸæ›´æ–° {successful} ä¸ªæ–‡ä»¶ï¼Œå¤±è´¥/è·³è¿‡ {failed} ä¸ªã€‚")
    finally:
        lock_file_path.unlink(missing_ok=True)
        logger.info("é”æ–‡ä»¶å·²æ¸…é™¤ã€‚")

if __name__ == "__main__":
    main()
