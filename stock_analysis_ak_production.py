# stock_analysis_pytdx_production.py - æœ€ç»ˆ pytdx (é€šè¾¾ä¿¡) ç¨³å®šç‰ˆ

import pandas as pd
import pandas_ta as ta
from datetime import datetime, timedelta
import pytz
from concurrent.futures import ThreadPoolExecutor
import time 

# --- æ–°å¢ pytdx ä¾èµ– ---
from pytdx.hq import TdxHq_API
from pytdx.exhq import TdxExHq_API
from pytdx.util import best_ip
from pytdx.errors import TdxConnectionError

# --- é¡¶éƒ¨æ–°å¢å¯¼å…¥ ---
import logging
from pathlib import Path
from tqdm import tqdm
import warnings
warnings.filterwarnings("ignore")
warnings.simplefilter(action='ignore', category=FutureWarning)

# --- å¸¸é‡å’Œé…ç½® ---
shanghai_tz = pytz.timezone('Asia/Shanghai')
OUTPUT_DIR = "index_data" 
DEFAULT_START_DATE = '2000-01-01' # pytdx æ¥å£éœ€è¦ YYYY-MM-DD æ ¼å¼
INDICATOR_LOOKBACK_DAYS = 30 
LOCK_FILE = "stock_analysis.lock" 

MAX_WORKERS = 1 
MAX_RETRIES = 3 # pytdx è¿æ¥é‡è¯•æ¬¡æ•°ï¼Œå°è¯• 3 æ¬¡

# é€šè¾¾ä¿¡æœåŠ¡å™¨ IP å’Œç«¯å£
TDX_SERVERS = [
    ('119.147.212.81', 7709), 
    ('119.147.212.81', 7721)  
]
# pytdx å‘¨æœŸæ˜ å°„: 9:æ—¥çº¿, 5:å‘¨çº¿, 6:æœˆçº¿, 8:1åˆ†é’Ÿ
TDX_FREQ_MAP = {'D': 9, 'W': 5, 'M': 6}

# å®šä¹‰æ‰€æœ‰ä¸»è¦ A è‚¡æŒ‡æ•°åˆ—è¡¨ (æ³¨æ„ï¼špytdx éœ€è¦ SH/SZ å¸‚åœºä»£ç )
INDEX_LIST = {
    '000001': {'name': 'ä¸Šè¯æŒ‡æ•°', 'market': 1}, 
    '399001': {'name': 'æ·±è¯æˆæŒ‡', 'market': 0}, 
    '399006': {'name': 'åˆ›ä¸šæ¿æŒ‡', 'market': 0},
    '000016': {'name': 'ä¸Šè¯50', 'market': 1}, 
    '000300': {'name': 'æ²ªæ·±300', 'market': 1}, 
    '000905': {'name': 'ä¸­è¯500', 'market': 1},
    '000852': {'name': 'ä¸­è¯1000', 'market': 1}, 
    '000688': {'name': 'ç§‘åˆ›50', 'market': 1}, 
    '399300': {'name': 'æ²ªæ·±300(æ·±)', 'market': 0},
    '000991': {'name': 'ä¸­è¯å…¨æŒ‡', 'market': 1},
    '000906': {'name': 'ä¸­è¯800', 'market': 1}, 
    '399005': {'name': 'ä¸­å°æ¿æŒ‡', 'market': 0}, 
    '399330': {'name': 'æ·±è¯100', 'market': 0},
    '000010': {'name': 'ä¸Šè¯180', 'market': 1}, 
    '000015': {'name': 'çº¢åˆ©æŒ‡æ•°', 'market': 1},
    '000011': {'name': 'ä¸Šè¯åŸºé‡‘æŒ‡æ•°', 'market': 1}, 
    '399305': {'name': 'æ·±è¯åŸºé‡‘æŒ‡æ•°', 'market': 0}, 
    '399306': {'name': 'æ·±è¯ETFæŒ‡æ•°', 'market': 0},
}

# --- é…ç½®æ—¥å¿—ç³»ç»Ÿ (ä¿æŒä¸å˜) ---
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)s | %(message)s',
    handlers=[
        logging.FileHandler("stock_analysis.log", encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# --- è¿æ¥å®¢æˆ·ç«¯ (è¿æ¥é‡è¯•) ---

def connect_tdx_api(servers):
    """å°è¯•è¿æ¥é€šè¾¾ä¿¡è¡Œæƒ… API"""
    api = TdxHq_API()
    for ip, port in servers:
        try:
            logger.info(f"   - å°è¯•è¿æ¥ pytdx æœåŠ¡å™¨: {ip}:{port}")
            if api.connect(ip, port):
                logger.info(f"   - è¿æ¥æˆåŠŸ: {ip}:{port}")
                return api
        except TdxConnectionError:
            logger.warning(f"   - è¿æ¥å¤±è´¥: {ip}:{port}")
    return None

# --- æŒ‡æ ‡è®¡ç®—å‡½æ•° (ä¿æŒä¸å˜) ---

def calculate_full_technical_indicators(df):
    """è®¡ç®—å®Œæ•´çš„æŠ€æœ¯æŒ‡æ ‡é›†ï¼šMA, RSI, KDJ, MACD, BBANDS, ATR, CCI, OBV"""
    if df.empty:
        return df
    
    df = df.set_index('date')
    # ... (è®¡ç®—é€»è¾‘ä¸ä¹‹å‰ç‰ˆæœ¬ç›¸åŒ)
    df.ta.sma(length=5, append=True, col_names=('MA5',))
    df.ta.sma(length=20, append=True, col_names=('MA20',))
    df.ta.rsi(length=14, append=True, col_names=('RSI14',))
    df.ta.stoch(k=9, d=3, smooth_k=3, append=True) 
    df = df.rename(columns={'STOCHk_9_3_3': 'K', 'STOCHd_9_3_3': 'D', 'STOCHj_9_3_3': 'J'})
    df.ta.macd(append=True)
    df = df.rename(columns={'MACD_12_26_9': 'MACD', 'MACDh_12_26_9': 'MACDh', 'MACDs_12_26_9': 'MACDs'})
    df.ta.bbands(length=20, std=2, append=True)
    df = df.rename(columns={
        'BBL_20_2.0': 'BB_lower', 'BBM_20_2.0': 'BB_middle', 'BBU_20_2.0': 'BB_upper',
        'BBB_20_2.0': 'BB_bandwidth', 'BBP_20_2.0': 'BB_percent'
    })
    df.ta.atr(length=14, append=True)
    df = df.rename(columns={'ATRr_14': 'ATR14'})
    df.ta.cci(length=20, append=True)
    df = df.rename(columns={'CCI_20_0.015': 'CCI20'})
    df.ta.obv(append=True)
    
    return df.reset_index()


def aggregate_and_analyze(df_raw_slice, freq, prefix):
    """æŒ‰é¢‘ç‡èšåˆæ•°æ®å¹¶è®¡ç®—æŒ‡æ ‡ (pytdx åŸå§‹æ•°æ®æ²¡æœ‰ turnover_rate)"""
    if df_raw_slice.empty:
        return pd.DataFrame()
        
    # pytdx åŸå§‹æ•°æ®æ²¡æœ‰ turnover_rateï¼Œç”¨ volume/amount ä¼°ç®—ï¼Œæˆ–ç›´æ¥ä¸èšåˆè¯¥åˆ—
    # è¿™é‡Œä¸ºäº†å…¼å®¹æ€§ï¼Œç®€å•å°†å…¶è®¾ç½®ä¸º NaNï¼Œåç»­æŒ‡æ ‡è®¡ç®—ä¸ä¼šä¾èµ–å®ƒ
    df_raw_slice['turnover_rate'] = float('nan')
    
    # æŒ‰ç…§ pytdx æ•°æ®çš„æ—¥æœŸå­—æ®µè¿›è¡Œé‡é‡‡æ ·
    agg_df = df_raw_slice.resample(freq).agg({
        'open': 'first', 'high': 'max', 'low': 'min', 'close': 'last',
        'vol': 'sum', 'turnover_rate': 'mean'
    }).dropna(subset=['close'])
    
    if not agg_df.empty:
         agg_df = agg_df.reset_index().rename(columns={'index': 'date', 'vol': 'volume'})
         agg_df = calculate_full_technical_indicators(agg_df)
         
         cols_to_keep = agg_df.columns.drop(['date', 'open', 'close', 'high', 'low', 'volume', 'turnover_rate'])
         agg_df = agg_df[['date'] + cols_to_keep.tolist()]
         agg_df = agg_df.rename(columns={col: f'{col}_{prefix}' for col in cols_to_keep})
         agg_df.set_index('date', inplace=True)
         
    return agg_df

# --- å¢é‡æ•°æ®è·å–ä¸åˆ†ææ ¸å¿ƒå‡½æ•° (ä½¿ç”¨ pytdx åˆ†é¡µ) ---

def get_full_history_data(api, market, code, freq):
    """
    ä½¿ç”¨ pytdx åˆ†é¡µè·å–å®Œæ•´çš„å†å² K çº¿æ•°æ®ã€‚
    pytdx å•æ¬¡æœ€å¤šè·å– 800 æ¡æ•°æ®ã€‚
    """
    all_data = []
    
    # ä»æœ€æ–°çš„æ•°æ®å¼€å§‹å¾€å‰åˆ†é¡µè·å–
    for start in range(0, 50000, 800): # é™åˆ¶æœ€å¤šè·å– 50000 æ¡æ•°æ® (çº¦ 200 å¹´ï¼Œå®‰å…¨é™åˆ¶)
        try:
            # pytdx.get_security_bars æ¯æ¬¡æœ€å¤šè·å– 800 æ¡æ•°æ®
            data = api.get_security_bars(freq, market, code, start, 800)
            
            if not data:
                break
            
            df = api.to_df(data)
            
            if df.empty:
                break
            
            # pytdx è¿”å›çš„æ•°æ®æ˜¯å€’åºçš„ï¼ˆæœ€æ–°æ•°æ®åœ¨æœ€å‰é¢ï¼‰ï¼Œè¿™é‡Œéœ€è¦å€’åºæ’åˆ—
            all_data.append(df)
            
            # å¦‚æœè·å–çš„æ•°æ®ä¸è¶³ 800 æ¡ï¼Œè¯´æ˜å·²ç»è·å–åˆ°æœ€æ—§çš„æ•°æ®ï¼Œå¯ä»¥åœæ­¢
            if len(df) < 800:
                break
            
        except Exception as e:
            logger.error(f"   - pytdx åˆ†é¡µè·å– {code} å¤±è´¥ (Start={start})ã€‚é”™è¯¯: {e}")
            break # å‡ºç°é”™è¯¯ç›´æ¥é€€å‡ºå¾ªç¯
            
    if all_data:
        # åˆå¹¶æ‰€æœ‰åˆ†é¡µæ•°æ®ï¼Œå¹¶å»é™¤é‡å¤
        df_combined = pd.concat(all_data, ignore_index=True)
        # å»é™¤é‡å¤è¡Œ (å¶å°”å‡ºç°)
        df_combined.drop_duplicates(subset=['datetime'], keep='first', inplace=True)
        # æŒ‰ç…§æ—¥æœŸå‡åºæ’åˆ— (æœ€æ—§åˆ°æœ€æ–°)
        df_combined.sort_values(by='datetime', inplace=True)
        
        # æ ¼å¼åŒ–æ—¥æœŸ
        df_combined['date'] = pd.to_datetime(df_combined['datetime']).dt.date
        df_combined.set_index('date', inplace=True)
        
        return df_combined
    return pd.DataFrame()


def get_and_analyze_data_slice(api, market, code, start_date):
    """
    è·å–æ•°æ®åˆ‡ç‰‡ (å› ä¸º pytdx æ²¡æœ‰æŒ‰æ—¥æœŸèŒƒå›´æŸ¥è¯¢çš„åŠŸèƒ½ï¼Œåªèƒ½å…¨é‡è·å–åæœ¬åœ°ç­›é€‰)ã€‚
    """
    logger.info(f"   - æ­£åœ¨è·å– {code} (pytdx æ¥å£) å…¨é‡æ•°æ®...")

    try:
        # 1. å…¨é‡è·å–æ•°æ®
        df_full = get_full_history_data(api, market, code, TDX_FREQ_MAP['D'])

        if df_full.empty:
            logger.warning(f"   - {code} æœªè·å–åˆ°æ•°æ®ã€‚")
            return None
        
        # 2. æœ¬åœ°ç­›é€‰ï¼ˆè·å–å¢é‡/é‡å åˆ‡ç‰‡ï¼‰
        start_dt = datetime.strptime(start_date, '%Y-%m-%d').date()
        df_raw = df_full[df_full.index >= start_dt].copy()

        if df_raw.empty:
            logger.warning(f"   - {code} ç­›é€‰ååˆ‡ç‰‡ä¸ºç©ºã€‚")
            return None
        
        # 3. pytdx æ•°æ®æ¸…æ´—å’Œé‡å‘½å
        df_raw.rename(columns={'vol': 'volume'}, inplace=True)
        
        # 4. æŒ‡æ ‡è®¡ç®—
        df_raw_processed = df_raw[['open', 'close', 'high', 'low', 'volume']].copy()
        df_raw_processed = df_raw_processed.reset_index()

        df_daily = calculate_full_technical_indicators(df_raw_processed.copy())
        
        # 5. å‘¨/æœˆ/å¹´æŒ‡æ ‡èšåˆè®¡ç®—ï¼ˆéœ€è¦åŸå§‹ close/high/low/vol æ•°æ®ï¼Œä¸” date è®¾ä¸º indexï¼‰
        df_raw.reset_index(inplace=True)
        df_raw['turnover_rate'] = float('nan') # å ä½
        df_raw.set_index('date', inplace=True)
        
        daily_cols = df_daily.columns.drop(['date', 'open', 'close', 'high', 'low', 'volume', 'turnover_rate'])
        df_daily = df_daily.rename(columns={col: f'{col}_D' for col in daily_cols})
        df_daily.set_index('date', inplace=True)
        
        # èšåˆå’Œåˆå¹¶
        df_weekly = aggregate_and_analyze(df_raw, 'W', 'W')
        df_monthly = aggregate_and_analyze(df_raw, 'M', 'M')
        df_yearly = aggregate_and_analyze(df_raw, 'Y', 'Y')

        results = df_daily.copy()
        results = results.join(df_weekly, how='left').join(df_monthly, how='left').join(df_yearly, how='left')
        results.index.name = 'date'
        
        logger.info(f"   - {code} æˆåŠŸåˆ†æ {len(results)} è¡Œæ•°æ®åˆ‡ç‰‡ã€‚")
        return results.sort_index()

    except Exception as e:
        logger.error(f"   - é”™è¯¯ï¼šå¤„ç†æŒ‡æ•° {code} å¤±è´¥ã€‚æœ€ç»ˆé”™è¯¯: {e}")
        return None

# --- å•ä¸ªæŒ‡æ•°å¤„ç†å’Œä¿å­˜å‡½æ•° (é€‚é… pytdx) ---

def process_single_index(api, code_map):
    """å¤„ç†å•ä¸ªæŒ‡æ•°ï¼Œå®ç°å¢é‡ä¸‹è½½ã€è®¡ç®—å’Œè¦†ç›–ä¿å­˜"""
    code = code_map['code']
    name = code_map['name']
    market = code_map['market']
    
    logger.info(f"-> æ­£åœ¨å¤„ç†æŒ‡æ•°: {code} ({name})")
    
    file_name = f"{code.replace('.', '_')}.csv"
    output_path = Path(OUTPUT_DIR) / file_name
    
    # pytdx éœ€è¦ YYYY-MM-DD æ ¼å¼
    start_date_to_request = DEFAULT_START_DATE
    df_old = pd.DataFrame()
    
    # 1. ç¡®å®šæœ¬æ¬¡ä¸‹è½½çš„èµ·å§‹æ—¥æœŸ 
    # (pytdx æ˜¯å…¨é‡è·å–ï¼Œè¿™é‡Œ start_date_to_request ä»…ç”¨äºæœ¬åœ°ç­›é€‰é‡å åˆ‡ç‰‡)
    if output_path.exists():
        try:
            # ... (è¯»å–æ—§æ–‡ä»¶é€»è¾‘ä¸å˜)
            df_old = pd.read_csv(output_path, index_col='date', parse_dates=True)
            if not df_old.empty:
                latest_date_in_repo = df_old.index.max()
                
                # å¾€å‰æ¨ INDICATOR_LOOKBACK_DAYS å¤©
                start_date_for_calc = latest_date_in_repo - timedelta(days=INDICATOR_LOOKBACK_DAYS)
                start_date_to_request = start_date_for_calc.strftime('%Y-%m-%d')
                
                if start_date_for_calc.strftime('%Y-%m-%d') < DEFAULT_START_DATE:
                     start_date_to_request = DEFAULT_START_DATE
                
                logger.info(f"   - æ£€æµ‹åˆ°æ—§æ•°æ®ï¼Œæœ€æ–°æ—¥æœŸä¸º {latest_date_in_repo.strftime('%Y-%m-%d')}ã€‚æœ¬åœ°ç­›é€‰ä» {start_date_to_request} å¼€å§‹çš„åˆ‡ç‰‡ï¼ˆå«é‡å ï¼‰ã€‚")
            else:
                logger.warning(f"   - æ—§æ–‡ä»¶ {output_path.name} ä¸ºç©ºï¼Œå°†å…¨é‡ä¸‹è½½ã€‚")
        except Exception as e:
            logger.error(f"   - è­¦å‘Šï¼šè¯»å–æ—§æ–‡ä»¶ {output_path.name} å¤±è´¥ ({e})ï¼Œå°†å…¨é‡ä¸‹è½½ã€‚")
            
    else:
        logger.info(f"   - æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°†å…¨é‡ä¸‹è½½ã€‚")


    # 2. è·å–æœ€æ–°æ•°æ®å’ŒæŒ‡æ ‡ (pytdx æ˜¯å…¨é‡è·å–åæœ¬åœ°ç­›é€‰)
    df_new_analyzed = get_and_analyze_data_slice(api, market, code, start_date_to_request)
    
    if df_new_analyzed is None:
        if not df_old.empty and df_old.index.max().date() == datetime.now(shanghai_tz).date():
             logger.info(f"   - {code} æ•°æ®å·²æ˜¯ä»Šå¤©æœ€æ–°ï¼Œè·³è¿‡ä¿å­˜ã€‚")
        else:
             logger.warning(f"   - {code} æœªè·å–åˆ°æ–°æ•°æ®ï¼Œä¿æŒåŸæ–‡ä»¶ã€‚")
        return False

    # 3. æ•´åˆæ–°æ—§æ•°æ® (æ—§æ•°æ®åªéœ€è¦ç­›é€‰å‡ºæ¯”æ–°åˆ‡ç‰‡æ›´æ—©çš„éƒ¨åˆ†)
    if not df_old.empty:
        old_data_to_keep = df_old[df_old.index < df_new_analyzed.index.min()]
    else:
        old_data_to_keep = pd.DataFrame()


    df_combined = pd.concat([old_data_to_keep, df_new_analyzed])
    # ç”±äºæˆ‘ä»¬æ‰‹åŠ¨ç­›é€‰äº†ï¼Œç†è®ºä¸Šä¸éœ€è¦å»é‡ï¼Œä½†ä¸ºå®‰å…¨ä¿ç•™
    results_to_save = df_combined[~df_combined.index.duplicated(keep='last')]
    results_to_save = results_to_save.sort_index()

    logger.info(f"   - âœ… {code} æˆåŠŸæ›´æ–°ã€‚æ€»è¡Œæ•°: {len(results_to_save)}")
    
    # 4. ä¿å­˜åˆ° CSV 
    results_to_save.to_csv(output_path, encoding='utf-8')
    return True

# --- ä¸»æ‰§è¡Œé€»è¾‘ ---
def main():
    start_time = time.time()
    output_path = Path(OUTPUT_DIR)
    
    # 1. æ£€æŸ¥è¿è¡Œé”
    lock_file_path = Path(LOCK_FILE)
    if lock_file_path.exists():
        logger.warning("æ£€æµ‹åˆ°é”æ–‡ä»¶ï¼Œè„šæœ¬å¯èƒ½æ­£åœ¨è¿è¡Œæˆ–ä¸Šæ¬¡å¼‚å¸¸é€€å‡ºã€‚ç»ˆæ­¢æœ¬æ¬¡è¿è¡Œã€‚")
        return
    lock_file_path.touch() 
    
    # 2. è¿æ¥ pytdx API
    tdx_api = None
    for attempt in range(MAX_RETRIES):
        tdx_api = connect_tdx_api(TDX_SERVERS)
        if tdx_api:
            break
        if attempt < MAX_RETRIES - 1:
            time.sleep(5)
    
    if not tdx_api:
        logger.error("âŒ æ— æ³•è¿æ¥åˆ°ä»»ä½• pytdx æœåŠ¡å™¨ï¼Œè„šæœ¬ç»ˆæ­¢ã€‚")
        lock_file_path.unlink(missing_ok=True)
        return
        
    try:
        # 3. åˆå§‹åŒ–ç›®å½•å’Œæ—¥å¿—
        output_path.mkdir(exist_ok=True) 
        logger.info("â€”" * 50)
        logger.info("ğŸš€ è„šæœ¬å¼€å§‹è¿è¡Œ (ä½¿ç”¨ pytdx)")
        logger.info(f"ç»“æœå°†ä¿å­˜åˆ°ä¸“ç”¨ç›®å½•: {output_path.resolve()}")
        logger.info(f"å‡†å¤‡ä¸²è¡Œå¤„ç† {len(INDEX_LIST)} ä¸ªä¸»è¦æŒ‡æ•°...")

        successful = 0
        failed = 0
        
        # 4. è½¬æ¢ INDEX_LIST æ ¼å¼ä»¥æ–¹ä¾¿å¤„ç†
        jobs = [{'code': code, **data} for code, data in INDEX_LIST.items()]
        
        # 5. ä½¿ç”¨ ThreadPoolExecutor è¿›è¡Œä¸²è¡Œå¤„ç† (MAX_WORKERS = 1)
        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            # æäº¤ä»»åŠ¡ï¼Œå°† API å®¢æˆ·ç«¯ä½œä¸ºå‚æ•°ä¼ å…¥
            futures = {
                executor.submit(process_single_index, tdx_api, job): job
                for job in jobs
            }
            
            # ä½¿ç”¨ tqdm åŒ…è£… futures å¾ªç¯
            for future in tqdm(futures, desc="å¤„ç†æŒ‡æ•°", unit="ä¸ª", ncols=100, leave=True):
                job = futures[future]
                try:
                    if future.result():
                        successful += 1
                    else:
                        failed += 1
                except Exception as e:
                    logger.error(f"å¤„ç† {job['code']} ({job['name']}) æ—¶å‘ç”Ÿæœªæ•è·å¼‚å¸¸: {e}")
                    failed += 1
        
        end_time = time.time()
        elapsed_time = end_time - start_time
        
        # 6. æœ€ç»ˆç»Ÿè®¡å’Œè¾“å‡º
        logger.info("â€”" * 50)
        logger.info(f"âœ… æ‰€æœ‰æŒ‡æ•°æ•°æ®å¤„ç†å®Œæˆã€‚æ€»è€—æ—¶: {elapsed_time:.2f} ç§’")
        logger.info(f"ç»Ÿè®¡ï¼šæˆåŠŸæ›´æ–° {successful} ä¸ªæ–‡ä»¶ï¼Œå¤±è´¥/è·³è¿‡ {failed} ä¸ªã€‚")

    finally:
        # 7. ç§»é™¤é”æ–‡ä»¶å¹¶æ–­å¼€è¿æ¥
        tdx_api.close()
        lock_file_path.unlink(missing_ok=True)
        logger.info("pytdx è¿æ¥å·²å…³é—­ï¼Œé”æ–‡ä»¶å·²æ¸…é™¤ã€‚")

if __name__ == "__main__":
    main()
