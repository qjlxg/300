# stock_analysis_ak_production.py - æœ€ç»ˆ EM ç¨³å®šç‰ˆï¼ˆlogging + pathlib + ç¨³å®šä¸²è¡Œï¼‰

import akshare as ak
import pandas as pd
import pandas_ta as ta
from datetime import datetime, timedelta
import os # ä¿ç•™ç”¨äºå…¼å®¹ ThreadPoolExecutor
import pytz
from concurrent.futures import ThreadPoolExecutor
import time 

# --- é¡¶éƒ¨æ–°å¢å¯¼å…¥ ---
import logging
from pathlib import Path
from tqdm import tqdm
import warnings
warnings.filterwarnings("ignore")

# --- å¸¸é‡å’Œé…ç½® ---
shanghai_tz = pytz.timezone('Asia/Shanghai')
OUTPUT_DIR = "index_data" 
DEFAULT_START_DATE = '20000101'
INDICATOR_LOOKBACK_DAYS = 30 
LOCK_FILE = "stock_analysis.lock" 

# å…³é”®è®¾ç½®ï¼šä¿æŒä¸²è¡Œï¼Œç¡®ä¿ç¨³å®šè¿è¡Œ
MAX_WORKERS = 1 
# æœ€å¤§é‡è¯•æ¬¡æ•°
MAX_RETRIES = 0 

# å®šä¹‰æ‰€æœ‰ä¸»è¦ A è‚¡æŒ‡æ•°åˆ—è¡¨
INDEX_LIST = {
    '000001': 'ä¸Šè¯æŒ‡æ•°', '399001': 'æ·±è¯æˆæŒ‡', '399006': 'åˆ›ä¸šæ¿æŒ‡',
    '000016': 'ä¸Šè¯50', '000300': 'æ²ªæ·±300', '000905': 'ä¸­è¯500',
    '000852': 'ä¸­è¯1000', '000688': 'ç§‘åˆ›50', '399300': 'æ²ªæ·±300(æ·±)',
    '000991': 'ä¸­è¯å…¨æŒ‡',
    '000906': 'ä¸­è¯800', '399005': 'ä¸­å°æ¿æŒ‡', '399330': 'æ·±è¯100',
    '000010': 'ä¸Šè¯180', '000015': 'çº¢åˆ©æŒ‡æ•°',
    '000011': 'ä¸Šè¯åŸºé‡‘æŒ‡æ•°', '399305': 'æ·±è¯åŸºé‡‘æŒ‡æ•°', '399306': 'æ·±è¯ETFæŒ‡æ•°',
}

# --- é…ç½®æ—¥å¿—ç³»ç»Ÿ (æ›¿ä»£ print) ---
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)s | %(message)s',
    handlers=[
        logging.FileHandler("stock_analysis.log", encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


# --- æŒ‡æ ‡è®¡ç®—å‡½æ•° (ä¿æŒä¸å˜) ---

def calculate_full_technical_indicators(df):
    """è®¡ç®—å®Œæ•´çš„æŠ€æœ¯æŒ‡æ ‡é›†ï¼šMA, RSI, KDJ, MACD, BBANDS, ATR, CCI, OBV"""
    if df.empty:
        return df
    
    df = df.set_index('date')

    # 1. ç§»åŠ¨å¹³å‡çº¿ (MA)
    df.ta.sma(length=5, append=True, col_names=('MA5',))
    df.ta.sma(length=20, append=True, col_names=('MA20',))
    # 2. ç›¸å¯¹å¼ºå¼±æŒ‡æ•° (RSI)
    df.ta.rsi(length=14, append=True, col_names=('RSI14',))
    # 3. éšæœºæŒ‡æ ‡ (KDJ)
    df.ta.stoch(k=9, d=3, smooth_k=3, append=True) 
    df = df.rename(columns={'STOCHk_9_3_3': 'K', 'STOCHd_9_3_3': 'D', 'STOCHj_9_3_3': 'J'})
    # 4. æŒ‡æ•°å¹³æ»‘ç§»åŠ¨å¹³å‡çº¿ (MACD)
    df.ta.macd(append=True)
    df = df.rename(columns={'MACD_12_26_9': 'MACD', 'MACDh_12_26_9': 'MACDh', 'MACDs_12_26_9': 'MACDs'})
    # 5. Bollinger Bands (BBANDS)
    df.ta.bbands(length=20, std=2, append=True)
    df = df.rename(columns={
        'BBL_20_2.0': 'BB_lower', 'BBM_20_2.0': 'BB_middle', 'BBU_20_2.0': 'BB_upper',
        'BBB_20_2.0': 'BB_bandwidth', 'BBP_20_2.0': 'BB_percent'
    })
    # 6. Average True Range (ATR)
    df.ta.atr(length=14, append=True)
    df = df.rename(columns={'ATRr_14': 'ATR14'})
    # 7. Commodity Channel Index (CCI)
    df.ta.cci(length=20, append=True)
    df = df.rename(columns={'CCI_20_0.015': 'CCI20'})
    # 8. On-Balance Volume (OBV)
    df.ta.obv(append=True)
    
    return df.reset_index()


def aggregate_and_analyze(df_raw_slice, freq, prefix):
    """æŒ‰é¢‘ç‡èšåˆæ•°æ®å¹¶è®¡ç®—æŒ‡æ ‡"""
    if df_raw_slice.empty:
        return pd.DataFrame()
        
    agg_df = df_raw_slice.resample(freq).agg({
        'open': 'first', 'high': 'max', 'low': 'min', 'close': 'last',
        'volume': 'sum', 'turnover_rate': 'mean'
    }).dropna()
    
    if not agg_df.empty:
         agg_df = agg_df.reset_index().rename(columns={'index': 'date'})
         agg_df = calculate_full_technical_indicators(agg_df)
         
         cols_to_keep = agg_df.columns.drop(['date', 'open', 'close', 'high', 'low', 'volume', 'turnover_rate'])
         agg_df = agg_df[['date'] + cols_to_keep.tolist()]
         agg_df = agg_df.rename(columns={col: f'{col}_{prefix}' for col in cols_to_keep})
         agg_df.set_index('date', inplace=True)
         
    return agg_df

# --- å¢é‡æ•°æ®è·å–ä¸åˆ†ææ ¸å¿ƒå‡½æ•° (ä½¿ç”¨ EM æ¥å£) ---

def get_and_analyze_data_slice(symbol, start_date):
    """
    ä½¿ç”¨ akshare çš„ä¸œæ–¹è´¢å¯Œæ¥å£ (index_em_hist) è·å–æ•°æ®ã€‚
    """
    end_date_str = datetime.now(shanghai_tz).strftime('%Y%m%d')
    logger.info(f"   - æ­£åœ¨è·å– {symbol} (EM æ¥å£) ä» {start_date} å¼€å§‹çš„æ•°æ®...")

    for attempt in range(1, MAX_RETRIES + 1):
        try:
            # 1. ã€å…³é”®ä¿®æ”¹ã€‘ï¼šä½¿ç”¨ä¸œæ–¹è´¢å¯Œæ¥å£ ak.index_em_hist
            df_raw = ak.index_em_hist(
                symbol=symbol, 
                start_date=start_date, 
                end_date=end_date_str,
                adjust='qfq' 
            )
            
            # æˆåŠŸè·å–ï¼Œè·³å‡ºé‡è¯•å¾ªç¯
            if df_raw.empty:
                logger.warning(f"   - {symbol} æœªè·å–åˆ°æ•°æ®ã€‚")
                return None
            
            # 2. ã€å…³é”®ä¿®æ”¹ã€‘ï¼šè°ƒæ•´åˆ—åä»¥åŒ¹é… EM æ¥å£è¿”å›çš„æ ¼å¼
            df_raw.columns = [
                'date', 'open', 'close', 'high', 'low', 'change_pct', 
                'change_abs', 'volume', 'amount', 'turnover_rate'
            ]
            
            # 3. æ•°æ®æ¸…æ´—ã€è®¡ç®—æŒ‡æ ‡å’Œåˆå¹¶ 
            df_raw['date'] = pd.to_datetime(df_raw['date'])
            df_raw_processed = df_raw[['date', 'open', 'close', 'high', 'low', 'volume', 'turnover_rate']].copy()
            
            # ... (åç»­è®¡ç®—æŒ‡æ ‡ã€åˆå¹¶é€»è¾‘ä¸å˜)
            df_daily = calculate_full_technical_indicators(df_raw_processed.copy())
            daily_cols = df_daily.columns.drop(['date', 'open', 'close', 'high', 'low', 'volume', 'turnover_rate'])
            df_daily = df_daily.rename(columns={col: f'{col}_D' for col in daily_cols})
            df_raw.set_index('date', inplace=True)
            df_weekly = aggregate_and_analyze(df_raw, 'W', 'W')
            df_monthly = aggregate_and_analyze(df_raw, 'M', 'M')
            df_yearly = aggregate_and_analyze(df_raw, 'Y', 'Y')
            df_daily.set_index('date', inplace=True)
            results = df_daily.copy()
            results = results.join(df_weekly, how='left').join(df_monthly, how='left').join(df_yearly, how='left')
            results.index.name = 'date'
            
            logger.info(f"   - {symbol} æˆåŠŸåˆ†æ {len(results)} è¡Œæ•°æ®åˆ‡ç‰‡ã€‚")
            return results.sort_index()

        # ç»Ÿä¸€å¼‚å¸¸å¤„ç† + æ›´ç²¾ç»†çš„é‡è¯•åˆ¤æ–­
        except Exception as e:
            error_msg = str(e)
            
            if attempt < MAX_RETRIES:
                is_connection_error = 'connection' in error_msg.lower() or 'remote disconnected' in error_msg.lower() or 'timeout' in error_msg.lower()
                
                if is_connection_error:
                    logger.warning(f"   - è·å– {symbol} æ•°æ®å¤±è´¥ (å°è¯• {attempt}/{MAX_RETRIES})ã€‚é”™è¯¯: è¿æ¥ä¸­æ–­/è¶…æ—¶ã€‚")
                    wait_time = 5 * attempt 
                    logger.info(f"   - æ­£åœ¨ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                    time.sleep(wait_time)
                else:
                    # éç½‘ç»œé”™è¯¯ï¼Œç›´æ¥è®°å½•å¹¶ç»ˆæ­¢é‡è¯•
                    logger.error(f"   - å¤„ç†æŒ‡æ•° {symbol} æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯ï¼Œç»ˆæ­¢é‡è¯•ã€‚é”™è¯¯: {e}")
                    return None
            else:
                logger.error(f"   - é”™è¯¯ï¼šå¤„ç†æŒ‡æ•° {symbol} è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ã€‚æœ€ç»ˆé”™è¯¯: {e}")
                return None

# --- å•ä¸ªæŒ‡æ•°å¤„ç†å’Œä¿å­˜å‡½æ•° (ä½¿ç”¨ logging + pathlib) ---

def process_single_index(code, name):
    """å¤„ç†å•ä¸ªæŒ‡æ•°ï¼Œå®ç°å¢é‡ä¸‹è½½ã€è®¡ç®—å’Œè¦†ç›–ä¿å­˜"""
    logger.info(f"-> æ­£åœ¨å¤„ç†æŒ‡æ•°: {code} ({name})")
    
    # ä½¿ç”¨ pathlib æ›¿ä»£ os.path
    file_name = f"{code.replace('.', '_')}.csv"
    output_path = Path(OUTPUT_DIR) / file_name
    
    start_date_to_request = DEFAULT_START_DATE
    df_old = pd.DataFrame()
    
    # 1. ç¡®å®šæœ¬æ¬¡ä¸‹è½½çš„èµ·å§‹æ—¥æœŸ (ä½¿ç”¨ pathlib.exists())
    if output_path.exists():
        try:
            df_old = pd.read_csv(output_path, index_col='date', parse_dates=True)
            if not df_old.empty:
                latest_date_in_repo = df_old.index.max()
                
                # å¾€å‰æ¨ INDICATOR_LOOKBACK_DAYS å¤©
                start_date_for_calc = latest_date_in_repo - timedelta(days=INDICATOR_LOOKBACK_DAYS)
                start_date_to_request = start_date_for_calc.strftime('%Y%m%d')
                
                if start_date_for_calc.strftime('%Y%m%d') < DEFAULT_START_DATE:
                     start_date_to_request = DEFAULT_START_DATE
                
                logger.info(f"   - æ£€æµ‹åˆ°æ—§æ•°æ®ï¼Œæœ€æ–°æ—¥æœŸä¸º {latest_date_in_repo.strftime('%Y-%m-%d')}ã€‚ä» {start_date_to_request} å¼€å§‹ä¸‹è½½å¢é‡æ•°æ®å—ï¼ˆå«é‡å ï¼‰ã€‚")
            else:
                logger.warning(f"   - æ—§æ–‡ä»¶ {output_path.name} ä¸ºç©ºï¼Œä» {DEFAULT_START_DATE} å¼€å§‹ä¸‹è½½æ‰€æœ‰å†å²æ•°æ®ã€‚")
        except Exception as e:
            logger.error(f"   - è­¦å‘Šï¼šè¯»å–æ—§æ–‡ä»¶ {output_path.name} å¤±è´¥ ({e})ï¼Œå°†ä» {DEFAULT_START_DATE} é‡æ–°ä¸‹è½½ã€‚")
            
    else:
        logger.info(f"   - æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä» {DEFAULT_START_DATE} å¼€å§‹ä¸‹è½½æ‰€æœ‰å†å²æ•°æ®ã€‚")


    # 2. è·å–æœ€æ–°æ•°æ®å’ŒæŒ‡æ ‡ (åªè·å–å¢é‡æ•°æ®å—)
    df_new_analyzed = get_and_analyze_data_slice(code, start_date_to_request)
    
    if df_new_analyzed is None:
        # ä¼˜åŒ–åˆ¤æ–­ï¼šå¦‚æœä»Šå¤©æ•°æ®å·²å­˜åœ¨ï¼Œè·³è¿‡
        if not df_old.empty and df_old.index.max().date() == datetime.now(shanghai_tz).date():
             logger.info(f"   - {code} æ•°æ®å·²æ˜¯ä»Šå¤©æœ€æ–°ï¼Œè·³è¿‡ä¿å­˜ã€‚")
        else:
             logger.warning(f"   - {code} æœªè·å–åˆ°æ–°æ•°æ®ï¼Œä¿æŒåŸæ–‡ä»¶ã€‚")
        return False

    # 3. æ•´åˆæ–°æ—§æ•°æ® 
    df_combined = pd.concat([df_old, df_new_analyzed])
    # ç§»é™¤ç´¢å¼•é‡å¤çš„è¡Œï¼Œä¿ç•™æœ€æ–°çš„åˆ†æç»“æœ (keep='last')
    results_to_save = df_combined[~df_combined.index.duplicated(keep='last')]
    results_to_save = results_to_save.sort_index()

    logger.info(f"   - âœ… {code} æˆåŠŸæ›´æ–°ã€‚æ€»è¡Œæ•°: {len(results_to_save)}")
    
    # 4. ä¿å­˜åˆ° CSV (è¦†ç›–æ—§æ–‡ä»¶)
    results_to_save.to_csv(output_path, encoding='utf-8')
    return True

# --- ä¸»æ‰§è¡Œé€»è¾‘ (å¸¦é”å’Œè¿›åº¦ç»Ÿè®¡) ---
def main():
    start_time = time.time()
    output_path = Path(OUTPUT_DIR)
    
    # 1. æ£€æŸ¥è¿è¡Œé”
    lock_file_path = Path(LOCK_FILE)
    if lock_file_path.exists():
        logger.warning("æ£€æµ‹åˆ°é”æ–‡ä»¶ï¼Œè„šæœ¬å¯èƒ½æ­£åœ¨è¿è¡Œæˆ–ä¸Šæ¬¡å¼‚å¸¸é€€å‡ºã€‚ç»ˆæ­¢æœ¬æ¬¡è¿è¡Œã€‚")
        return
    
    lock_file_path.touch() # åˆ›å»ºé”æ–‡ä»¶
    
    try:
        # 2. åˆå§‹åŒ–ç›®å½•å’Œæ—¥å¿—
        output_path.mkdir(exist_ok=True) # ä½¿ç”¨ pathlib æ›¿ä»£ os.makedirs
        logger.info("â€”" * 50)
        logger.info("ğŸš€ è„šæœ¬å¼€å§‹è¿è¡Œ")
        logger.info(f"ç»“æœå°†ä¿å­˜åˆ°ä¸“ç”¨ç›®å½•: {output_path.resolve()}")
        logger.info(f"å‡†å¤‡ä¸²è¡Œå¤„ç† {len(INDEX_LIST)} ä¸ªä¸»è¦æŒ‡æ•°...")

        successful = 0
        failed = 0
        
        # 3. ä½¿ç”¨ ThreadPoolExecutor è¿›è¡Œä¸²è¡Œå¤„ç† (MAX_WORKERS = 1)
        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            # æäº¤ä»»åŠ¡ï¼Œå¹¶æ„å»ºå­—å…¸ç”¨äº tracking
            futures = {
                executor.submit(process_single_index, code, name): (code, name)
                for code, name in INDEX_LIST.items()
            }
            
            # ä½¿ç”¨ tqdm åŒ…è£… futures å¾ªç¯ï¼Œæä¾›è¿›åº¦åé¦ˆ
            for future in tqdm(futures, desc="å¤„ç†æŒ‡æ•°", unit="ä¸ª", ncols=100, leave=True):
                code, name = futures[future]
                try:
                    # è·å–ç»“æœï¼Œå¦‚æœä¸º True åˆ™æˆåŠŸ
                    if future.result():
                        successful += 1
                    else:
                        # ç»“æœä¸º Falseï¼ˆå¦‚æœªè·å–åˆ°æ–°æ•°æ®æˆ–é‡è¯•å¤±è´¥ï¼‰
                        failed += 1
                except Exception as e:
                    logger.error(f"å¤„ç† {code} ({name}) æ—¶å‘ç”Ÿæœªæ•è·å¼‚å¸¸: {e}")
                    failed += 1
        
        end_time = time.time()
        elapsed_time = end_time - start_time
        
        # 4. æœ€ç»ˆç»Ÿè®¡å’Œè¾“å‡º
        logger.info("â€”" * 50)
        logger.info(f"âœ… æ‰€æœ‰æŒ‡æ•°æ•°æ®å¤„ç†å®Œæˆã€‚æ€»è€—æ—¶: {elapsed_time:.2f} ç§’")
        logger.info(f"ç»Ÿè®¡ï¼šæˆåŠŸæ›´æ–° {successful} ä¸ªæ–‡ä»¶ï¼Œå¤±è´¥/è·³è¿‡ {failed} ä¸ªã€‚")

    finally:
        # 5. ç§»é™¤é”æ–‡ä»¶ (æ— è®ºæˆåŠŸå¤±è´¥éƒ½æ‰§è¡Œ)
        lock_file_path.unlink(missing_ok=True)
        logger.info("é”æ–‡ä»¶å·²æ¸…é™¤ã€‚")

if __name__ == "__main__":
    main()
